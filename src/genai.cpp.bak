/******************************************************************************************************************************************
* Design considerations:
* 
* Designing an ideal structure for a neural network in C++ depends on various factors, including the specific requirements, 
* complexity of the network, and the desired level of flexibility. However, here are some key components and considerations 
* for building an efficient and modular neural network structure in C++:
* 
* Layer Abstraction: Define a base layer class that represents a generic layer in the neural network. This class should have 
* common methods like forward, backward, and updateWeights for performing forward propagation, backpropagation, and weight updates.
*
* Different Layer Implementations: Implement various types of layers as derived classes, such as fully connected layers, convolutional 
* layers, recurrent layers, activation layers, etc. Each layer implementation should inherit from the base layer class and provide 
* specific implementations for the required methods.
*
* Network Architecture: Define a network class that represents the overall neural network structure. It should encapsulate multiple 
* layers and provide methods for adding layers, connecting them, and performing forward and backward passes.
*
* Input and Output Handling: Consider how input data will be fed to the network and how output predictions will be obtained. Design 
* appropriate interfaces for loading and preprocessing input data and handling the output predictions.
*
* Training and Optimization: Implement training algorithms like stochastic gradient descent (SGD), mini-batch gradient descent, or more 
* advanced optimization methods. Include methods for setting up training parameters, initializing weights, and updating them during training.
* 
* Serialization and Persistence: Provide methods for saving and loading trained models to disk, allowing you to reuse or deploy the trained 
* network without retraining.
* 
* Integration with Libraries: Utilize existing C++ libraries for efficient numerical computations, such as BLAS libraries for matrix 
* operations or CUDA for GPU acceleration. Incorporate these libraries into your network implementation to improve performance.
* 
* Modularity and Flexibility: Design the structure in a modular and flexible way, allowing easy extension and customization. Consider 
* incorporating design patterns like Factory, Builder, or Strategy to enhance flexibility and reusability.
*
* System Design:
* Model Parallelism: Given the massive size of the model, it might be necessary to partition the parameters across multiple GPUs or devices 
* to enable efficient training. Model parallelism techniques, such as pipeline parallelism or model slicing, could be employed.
* 
* Distributed Training: Training a model with 175 billion parameters would likely require distributing the workload across multiple machines. 
* Distributed training frameworks like TensorFlow's Distributed Strategy or PyTorch's DistributedDataParallel can help parallelize the computations 
* and synchronize gradients across machines.
* 
* Cluster Infrastructure: A large-scale model would require a powerful cluster infrastructure with high-performance GPUs or specialized hardware 
* like TPUs. The number of machines and GPUs would depend on the specific requirements and scalability needs.
*
* Data Pipeline and Preprocessing: Handling large datasets efficiently is crucial. Designing a robust and scalable data pipeline that can 
* preprocess and load data in parallel is essential for training such models. Techniques like distributed data loading and data sharding can be employed.
*
* Model Architecture: The specific architecture of the model would depend on the task it aims to solve. For natural language processing tasks, 
* architectures like transformers have been successful. However, with 175 billion parameters, the architecture might involve complex variations, 
* deep hierarchies, and advanced attention mechanisms.
*
* Parameter Server or All-Reduce Approach: Coordinating the model parameters across different devices or machines can be done using a parameter 
* server architecture or an all-reduce approach. Both approaches have their trade-offs in terms of communication overhead and synchronization efficiency.
*
* Deployment Considerations: Deploying a large-scale model requires careful engineering and optimization to ensure efficient inference. 
* Techniques like model pruning, quantization, or specialized hardware deployment (e.g., using TensorRT or ONNX Runtime) might be considered to 
* improve inference speed and resource utilization.
*
* Monitoring and Scalability: Monitoring the training process, tracking model performance, and ensuring scalability are critical. Tools like 
* TensorFlow Extended (TFX) or Kubeflow can help with managing large-scale training pipelines and monitoring system metrics.
* 
* It's important to note that the above considerations provide a broad overview and may vary depending on the specific requirements, constraints, 
* and resources available. Designing a system for a model with 175 billion parameters is a highly complex task that requires deep expertise in 
* distributed systems, machine learning, and infrastructure design.
* 
* Attention Mechanism Considerations:
* Advanced attention mechanisms refer to more sophisticated and enhanced variations of the traditional attention mechanism. These variations 
* aim to address specific challenges or improve the effectiveness of attention-based models in different tasks. Here are a few examples:
* 
* Multi-head Attention: Instead of using a single attention mechanism, multi-head attention employs multiple parallel attention mechanisms 
* operating on different subspaces of the input. This allows the model to capture different types of information and learn more diverse representations.
*
* Self-Attention with Masking: In certain tasks, such as machine translation or language generation, it is essential to mask certain 
* positions in the input sequence to prevent the model from attending to future or unseen information. Masked self-attention ensures that 
* attention is only applied to valid positions in the sequence, taking into account the autoregressive nature of the task.
* 
* Relative Positional Encodings: Positional encodings are often used in attention mechanisms to incorporate positional information into the model. 
* Advanced attention mechanisms introduce relative positional encodings that capture relative distances or relationships between positions, 
* enabling the model to better understand the sequential structure of the input.
*
* Sparse Attention: In models with a large number of parameters, computing attention weights for all possible pairwise interactions can be 
* computationally expensive. Sparse attention mechanisms aim to reduce this computational burden by approximating the attention weights only for 
* a subset of the elements, typically based on their proximity or relevance to each other.
*
* Structured Attention: Traditional attention mechanisms operate on sequential or tabular data. However, in some domains, such as graphs or 
* images, attention needs to be applied to structured data. Advanced attention mechanisms adapt the attention mechanism to incorporate the 
* structural properties of the data, enabling the model to capture dependencies and relationships in a structured manner.
*
* Parameter server Approach (Distributed Training Strategies)
* Parameter server and all-reduce are two commonly used distributed training strategies in deep learning. Here's a comparison between 
* the two approaches:
* 
* Communication Pattern:
* 
* Parameter Server: In the parameter server approach, the model parameters are divided and stored on separate parameter servers. During 
* training, workers (e.g., GPUs or machines) communicate with the parameter servers to read and update the parameters.
* All-reduce: In the all-reduce approach, all workers participate in collective communication operations to synchronize their model parameters. 
* Each worker computes gradients locally, and all workers collectively reduce and update the gradients to ensure consistency across all replicas.
* Communication Overhead:
*
* Parameter Server: The parameter server approach involves communication between workers and parameter servers during parameter read and update 
* operations. The frequency of communication can be higher compared to all-reduce, especially when there are a large number of parameter servers 
* or when parameter updates are frequent.
* All-reduce: All-reduce involves communication among all workers during the gradient reduction step. The frequency of communication is 
* typically lower compared to parameter server, as workers exchange gradients periodically rather than after each update.
* Scalability:
* 
* Parameter Server: Parameter server architectures can scale to a large number of workers, as each worker communicates with a subset of parameter 
* servers. However, the scalability is limited by the communication bandwidth and latency between workers and parameter servers.
* All-reduce: All-reduce can scale efficiently to a large number of workers, as all workers participate in the collective communication. 
* The scalability of all-reduce depends on the network topology and the efficiency of the collective communication implementation.
* Fault Tolerance:

* Parameter Server: Parameter server architectures may suffer from single points of failure if a parameter server becomes unavailable. 
* Fault tolerance can be achieved by replicating the parameter servers or implementing backup mechanisms.
* All-reduce: All-reduce is inherently fault-tolerant as it relies on collective communication among all workers. If a worker fails, the 
* remaining workers can continue the training process.
* Memory and Storage Requirements:
* 
* Parameter Server: Parameter server architectures require storage for the model parameters on the parameter servers. The storage requirements 
* depend on the size of the model and the number of parameter servers.
* All-reduce: All-reduce requires memory for storing gradients during the reduction step. The memory requirements depend on the size of the 
* model and the number of workers.
* Both parameter server and all-reduce approaches have their strengths and weaknesses, and the choice depends on various factors such as the 
* size of the model, the number of workers, communication overhead, and fault tolerance requirements. In recent years, all-reduce has gained 
* popularity due to its scalability, fault tolerance, and efficient utilization of resources in distributed deep learning training.
*
* In terms of All-reduce approach:
* All-reduce algorithms, such as ring-based or tree-based algorithms, have demonstrated good scalability and efficiency in synchronous 
* gradient aggregation, reducing the communication overhead compared to parameter server architectures. This has made all-reduce more 
* attractive for large-scale distributed training, especially in scenarios with a large number of workers or when training large models.
* 
* However, it's important to note that the field of deep learning and distributed training is rapidly evolving. New techniques, frameworks, 
* and approaches continue to emerge, and the choice of distributed training strategy may vary depending on the specific requirements and 
* constraints of the training task.
* 
* To have the most up-to-date information on the current trends and practices in distributed deep learning training, I would recommend 
* referring to recent research papers, industry practices, and consulting with experts in the field.
*
* Ring-based and tree-based algorithms are two common approaches used in distributed computing, including in the context of all-reduce 
* operations in distributed deep learning. Here's a brief comparison of the two:
*
* Ring-based Algorithm:
*
* In a ring-based algorithm, the workers are arranged in a logical ring structure.
* The data is passed sequentially from one worker to the next in a circular manner until it reaches all the workers.
* Each worker performs a local reduction operation on the data it receives and then forwards the result to the next worker.
* The process continues until the data has been reduced by all workers and returned to the original sender.
* Ring-based algorithms are relatively simple to implement and have low latency but may suffer from load imbalance if the computation or communication 
* times vary significantly between workers.
*
* Tree-based Algorithm:
* 
* In a tree-based algorithm, the workers are organized in a hierarchical tree structure.
* The data is aggregated in a hierarchical manner, starting from the leaf nodes and moving up to the root node.
* Each node in the tree combines the data from its child nodes and performs a reduction operation.
* The process continues recursively until the root node receives the final reduced data.
* Tree-based algorithms can provide better load balancing compared to ring-based algorithms as the data aggregation happens in a 
* hierarchical structure.
* However, they may introduce higher latency due to additional communication steps involved in traversing the tree structure.
* The choice between ring-based and tree-based algorithms depends on various factors, such as the number of workers, the communication infrastructure, 
* and the characteristics of the training workload. Both algorithms have their strengths and weaknesses, and their performance can vary based on 
* the specific system and workload conditions.
* 
* It's worth noting that there are also other variations and optimizations of all-reduce algorithms, such as recursive doubling, butterfly, and more, 
* which aim to improve performance in different contexts. The choice of the most suitable algorithm often requires experimentation and benchmarking 
* on the target system to find the optimal configuration for a given distributed training task.
* 
* Here is a list of some commonly used variations and optimizations of all-reduce algorithms:
* 
* Ring-Based Algorithms: Traditional ring-based algorithms are widely used and serve as the baseline for many other algorithms.
* 
* Tree-Based Algorithms: Tree-based algorithms, such as binomial tree, k-ary tree, or hypercube-based tree, provide better load balancing and 
* reduced communication steps compared to ring-based algorithms.
* 
* Recursive Doubling: Recursive doubling algorithms leverage the binary representation of the rank to perform reduction operations in a hierarchical 
* manner, effectively reducing the number of communication steps.
* 
* Butterfly Algorithm: The butterfly algorithm uses a combination of butterfly networks and hypercube networks to achieve reduced latency and 
* improved bandwidth utilization.
* 
* AllGather: AllGather is an extension of all-reduce that collects the input data from all workers onto every worker, rather than performing a 
* reduction operation. It is commonly used for gathering statistics or exchanging information across all workers.
* 
* AllReduce-Multi: AllReduce-Multi algorithms allow simultaneous communication of multiple smaller messages instead of a single large message, which 
* can improve performance in certain scenarios, especially when dealing with heterogeneous network environments.
* 
* Gradient Compression: Gradient compression techniques, such as top-K sparsification or quantization, can be applied to reduce the communication 
* bandwidth and latency during the all-reduce operation while still maintaining reasonable model accuracy.
* 
* Ring All-Reduce with All-Gather: This approach combines the ring-based all-reduce with an all-gather operation to reduce the overall communication 
* time, especially when the number of workers is large.
*
* Gradient Accumulation: Gradient accumulation techniques allow workers to accumulate gradients over multiple iterations before performing the 
* all-reduce operation, reducing the frequency of communication and potentially improving scalability.
* 
* Asynchronous All-Reduce: Asynchronous algorithms, such as asynchronous decentralized parallel stochastic gradient descent (A-DePSGD), relax 
* the synchronization requirements and overlap communication with computation to improve overall training throughput.
* 
* These are just a few examples of the variations and optimizations available for all-reduce algorithms. The choice of which algorithm to use 
* depends on factors such as network topology, system characteristics, workload, and communication patterns, and it often requires careful 
* experimentation and benchmarking to identify the best approach for a specific distributed training scenario.
*
* In terms of Training distribution:
* Distributing training across multiple workers in a C++ codebase involves partitioning the data and model parameters, performing computations 
* on each worker, and synchronizing the updates to keep the model consistent. Here's a high-level overview of how training can be broken 
* down and distributed across workers:
* 
* Data Partitioning: Split the training data into multiple shards or subsets, where each worker is responsible for processing a different portion 
* of the data. The data can be partitioned based on samples, batches, or other appropriate criteria.
* 
* Model Replication: Replicate the model parameters on each worker. This ensures that each worker has a copy of the complete model for performing 
* computations independently.
* 
* Forward Pass: Each worker performs a forward pass on its local data subset using the replicated model. The forward pass computes the predictions 
* and loss for the local data.
* 
* Backward Pass and Gradient Computation: After the forward pass, each worker computes the gradients of the model parameters with respect to the 
* local data subset. The gradients can be computed using techniques like backpropagation or automatic differentiation.
* 
* Gradient Aggregation: The computed gradients from each worker need to be aggregated to obtain a global gradient. This can be done using various 
* aggregation algorithms, such as the All-Reduce algorithm, where gradients are exchanged and combined across workers to compute the average or sum 
* of gradients.
* 
* Parameter Update: Once the global gradient is obtained, each worker updates its local copy of the model parameters using an optimization algorithm, 
* such as stochastic gradient descent (SGD) or Adam. The updates can be applied asynchronously or synchronously based on the distributed training strategy.
* 
* Synchronization: If training is performed asynchronously, it is necessary to periodically synchronize the model parameters across workers to maintain 
* consistency. Synchronization can be done by broadcasting the updated model parameters from a designated worker to other workers.
*
* Iterative Training: The above steps are repeated for multiple iterations or epochs until the desired convergence or training criteria are met. Each 
* iteration involves data partitioning, forward and backward passes, gradient aggregation, parameter updates, and synchronization.
*
* It's important to note that the implementation details of distributed training in C++ may vary depending on the specific framework or library being 
* used. Popular frameworks like TensorFlow or PyTorch provide built-in support for distributed training with their own APIs and abstractions. 
* These frameworks handle the underlying communication, synchronization, and parameter updates across workers, allowing you to focus more on 
* defining the model and training process.
*
* When implementing distributed training in C++, you may need to utilize distributed computing libraries, such as MPI (Message Passing Interface) or 
* specialized distributed frameworks like Horovod, to facilitate inter-worker communication and coordination. These libraries provide functions and 
* utilities for message passing, collective operations, and distributed training patterns.
* 
* Overall, the process of distributing training across workers in C++ involves partitioning data, replicating the model parameters, performing 
* computations on each worker, aggregating gradients, updating parameters, and ensuring synchronization to achieve distributed training and collaboration.
******************************************************************************************************************************************/

#include <iostream>
#include <vector>
#include <unordered_map>
#include <unordered_set>
#include <cmath>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/embed.h>
#include <pybind11/numpy.h>
#include <cblas.h>
#include <omp.h>
#include <mpi.h>

namespace py = pybind11;

void print_string(const std::string& text, bool printNextLine) {
    if (printNextLine) {
        py::print(text);
    } else {
        py::print(text, py::arg("end") = "");
    }
}

void print_double(double value, bool printNextLine) {
    if (printNextLine) {
        py::print(value);
    } else {
        py::print(value, py::arg("end") = "");
    }
}

double* allocate_matrix(ssize_t rows, ssize_t cols) {
    // Allocate memory for the matrix
    double* matrix = (double*)malloc(rows * cols * sizeof(double));
    return matrix;
}

void process_array1(py::array_t<double> inputArray) {
    // Access the underlying NumPy array data
    py::buffer_info bufInfo = inputArray.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    // Access the shape and strides of the array
    std::vector<size_t> shape(bufInfo.shape.begin(), bufInfo.shape.end());
    std::vector<size_t> strides(bufInfo.strides.begin(), bufInfo.strides.end());

    // Iterate over the array elements
    for (size_t i = 0; i < shape[0]; ++i) {
        for (size_t j = 0; j < shape[1]; ++j) {
            // Access the element at (i, j)
            double value = dataPtr[i * strides[0] + j * strides[1]];
            // Process the element as needed
            // ...
            print_double(value, true);
        }
    }
}

void process_array(py::array_t<double> inputArray) {
    py::print("Processing array:");

/*
    auto bufInfo = inputArray.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    ssize_t size = bufInfo.size;

    for (ssize_t i = 0; i < size; ++i) {
        double value = dataPtr[i];
        py::print(value);
    }
*/

    py::buffer_info bufInfo = inputArray.request();

    // Get the shape of the array
    std::vector<size_t> shape(bufInfo.shape.begin(), bufInfo.shape.end());

    // Get the shape of the array
    // std::vector<size_t> shape = bufInfo.shape;

    // Determine the number of dimensions
    size_t numDimensions = shape.size();

    // Print the dimensions
    py::print("Number of dimensions:", numDimensions);
    py::print("Shape:", shape);

}

void process_matrix(py::array_t<double> inputMatrix) {
    py::print("Processing matrix:");

    auto bufInfo = inputMatrix.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    ssize_t rows = bufInfo.shape[0];
    ssize_t cols = bufInfo.shape[1];

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = dataPtr[i * cols + j];
            // py::print(value);
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }
}

py::array_t<double>  matmul(py::array_t<double> A, py::array_t<double> B) {
    py::print("Processing matrix:");

    auto bufInfoA = A.request();
    double* matA = static_cast<double*>(bufInfoA.ptr);

    auto bufInfoB = B.request();
    double* matB = static_cast<double*>(bufInfoB.ptr);

    ssize_t rows = bufInfoA.shape[0];
    ssize_t cols = bufInfoA.shape[1];

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = matA[i * cols + j];
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = matB[i * cols + j];
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }

    int rows_a = rows;
    int cols_a = cols;
    int cols_b = cols;

    // Create a new NumPy array with the same shape as the matrix
    // This actually allocates memory.
    py::array_t<double> result({rows, cols});

    // Get a pointer to the underlying data of the NumPy array
    double* matC = result.mutable_data();

    // otherwise, allocate manually
    // double* matC = allocate_matrix(rows, cols);

    float alpha = 1.0;
    float beta  = 0.0;
    cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_a, cols_b, cols_a, alpha,
                (double *) matA, cols_a,
                (double *) matB, cols_b, beta,
                (double *) matC, cols_b);

    return result;
}


class DAG {
private:
    std::unordered_map<int, std::unordered_set<int>> adjacencyList;

public:
    void addEdge(int source, int destination) {
        adjacencyList[source].insert(destination);
        adjacencyList[destination]; // Ensure destination node exists in the adjacency list
    }

    void printDAG() {
        for (const auto& entry : adjacencyList) {
            int source = entry.first;
            const std::unordered_set<int>& destinations = entry.second;

            print_string("Node " + std::to_string(source) + " -> ", false);
            for (int destination : destinations) {
                print_string(std::to_string(destination) + " ", false);
            }
            print_string(" ", true);
        }
    }
};

class Tensor {

private:
    std::vector<double> data;
    std::vector<Tensor*> inputs;
    std::vector<Tensor*> outputs;

public:
    Tensor() {}

    Tensor(const std::vector<double>& data) : data(data) {}

    void addInput(Tensor* input) {
        inputs.push_back(input);
        input->addOutput(this);
    }

    void addOutput(Tensor* output) {
        outputs.push_back(output);
    }

    void forward() {
        // Perform forward computation here
        // For simplicity, let's assume it just doubles the values
        for (double& value : data) {
            value *= 2;
        }

        // Propagate the computation to the connected tensors
        for (Tensor* output : outputs) {
            output->forward();
        }
    }

    void printData() {
        print_string("Tensor data:", false);
        for (double value : data) {
            print_double(value, false);
            print_string(" ", false);
        }
        print_string(" ", true);
    }

};


/********************************************************************************************
* NodeFactory
********************************************************************************************/

enum class NodeType {
    Input,
    Hidden,
    Output
};

class Node {
private:
    std::unordered_set<Node*> outputs;
    std::unordered_set<Node*> inputs;
    std::vector<double> values;
    std::vector<double> gradients; // Only for nodes requiring gradient calculations

public:
    std::string name;
    NodeType type;

    Node(const std::string& name, NodeType type, const std::vector<double>& initialValues = {})
        : name(name), type(type) {
        if (!initialValues.empty()) {
            setInitialValues(initialValues);
        }
    }

    void addInput(Node* input) {
        inputs.insert(input);
        input->outputs.insert(this);
    }

    void addOutput(Node* output) {
        outputs.insert(output);
        output->inputs.insert(this);
    }

    void setInitialValues(const std::vector<double>& initialValues) {
        values = initialValues;
print_string(name + " set " + std::to_string(values.size()), true);
        gradients = std::vector<double>(values.size(), 0.0);
    }

    void forward() {
        // Perform forward computation based on node type
        if (type == NodeType::Input) {
            // Input node simply outputs its values
            // No computation needed
        } else {
            // Compute the value based on inputs
            // values.clear();
            for (Node* input : inputs) {
                values.push_back(input->values[0]);
            }

            // Perform specific computations based on node type
            if (type == NodeType::Hidden) {
                // Perform some computation for hidden node
                // e.g., multiply input values by 2
                for (double& value : values) {
                    value *= 2;
                }
            } else if (type == NodeType::Output) {
                // Perform some computation for output node
                // e.g., sum all input values
                double sum = 0.0;
                for (double value : values) {
                    sum += value;
                }
                values = { sum };
            }
        }

        // Propagate forward to connected nodes
        for (Node* output : outputs) {
            output->forward();
        }
    }

    void backward() {
        // Initialize gradients
print_string(name + " oset " + std::to_string(values.size()), true);

        // Perform backward computation based on node type
        if (type == NodeType::Output) {
            // Output node gradients are computed based on some external criterion
            // Here, we simply set the gradient to 1
            gradients = std::vector<double>(values.size(), 0.0);
            gradients[0] = 1.0;
        } else {
            double sum = 0.0;
            // Compute gradients based on gradients received from connected nodes
            for (Node* output : outputs) {
                double outputGradient = output->gradients[0];
                size_t index = 0;
                for (auto input : output->inputs) {
                    if (input == this) {
                        gradients[index] += outputGradient * values[index];
                        break;
                    }
                    index++;
                }
            }
        }

        // Propagate gradients backward to connected nodes
        for (Node* input : inputs) {
            input->backward();
        }
    }

    void printValues() {
        print_string(name + " values: ", false);
        for (double value : values) {
            print_double(value, false);
        }
        print_string(" ", true);
    }

    void printGradients() {
        print_string(name + " gradients: ", false);
        for (double gradient : gradients) {
            print_double(gradient, false);
        }
        print_string(" ", true);
    }


};

class Graph {
private:
    std::vector<Node*> nodes;
    std::unordered_map<Node*, std::unordered_set<Node*>> adjacencyList;
    std::unordered_map<Node*, std::unordered_set<Node*>> reverseAdjacencyList;

public:

    // Create a node with three arguments: name, type, and initial values
    Node* createNode(const std::string& name, NodeType type, const std::vector<double>& initialValues) {
        Node* node = new Node(name, type, initialValues);
        nodes.push_back(node);
        adjacencyList[node] = std::unordered_set<Node*>();
        reverseAdjacencyList[node] = std::unordered_set<Node*>();
        return node;
    }

    // Create a node with two arguments: name and type (no initial values)
    Node* createNode(const std::string& name, NodeType type) {
        Node* node = new Node(name, type);
        nodes.push_back(node);
        adjacencyList[node] = std::unordered_set<Node*>();
        reverseAdjacencyList[node] = std::unordered_set<Node*>();
        return node;
    }

    void connect(Node* from, Node* to) {
        adjacencyList[from].insert(to);
        reverseAdjacencyList[to].insert(from); // Add reverse connection
    }

    void connect(Nodes** from_nodes, Node* to) {
        for (Node* node : from_nodes) {
           adjacencyList[node].insert(to);
           reverseAdjacencyList[to].insert(node); // Add reverse connection
        }
    }

    void connectNodes(Node* from, Node* to) {
        adjacencyList[from].insert(to);
        reverseAdjacencyList[to].insert(from); // Add reverse connection
    }

    void forwardPropagation() {
        std::unordered_set<Node*> visited;
        for (Node* node : nodes) {
            if (visited.find(node) == visited.end()) {
                forwardPropagationDFS(node, visited);
            }
        }
    }

    void forwardPropagationDFS(Node* node, std::unordered_set<Node*>& visited) {
        visited.insert(node);
        for (Node* output : adjacencyList[node]) {
            if (visited.find(output) == visited.end()) {
                forwardPropagationDFS(output, visited);
            }
        }
        node->forward();
    }

    void backwardPropagation() {
        std::unordered_set<Node*> visited;
        for (Node* node : nodes) {
            if (visited.find(node) == visited.end()) {
                backwardPropagationDFS(node, visited);
            }
        }
    }

    void backwardPropagationDFS(Node* node, std::unordered_set<Node*>& visited) {
        visited.insert(node);
        for (Node* input : reverseAdjacencyList[node]) {
            if (visited.find(input) == visited.end()) {
                backwardPropagationDFS(input, visited);
            }
        }
        node->backward();
    }

};

class Operator {
private:
    std::vector<Node*> operations;

public:

    py::array_t<double>  matmul(py::array_t<double> A, py::array_t<double> B) { 
        return ::matmul(A, B); // use global function
    }

    py::array_t<double>  linear(py::array_t<double> A, py::array_t<double> B) { 
        return ::matmul(A, B); // use global function
    }

    py::array_t<double>  normalize(py::array_t<double> A, py::array_t<double> B) { 
        py::array_t<double> result({10, 20});
        return result;
    }

    py::array_t<double>  mask(py::array_t<double> A, py::array_t<double> B) { 
        py::array_t<double> result({10, 20});
        return result;
    }

    py::array_t<double>  softmax(py::array_t<double> A, py::array_t<double> B) { 
        py::array_t<double> result({10, 20});
        return result;
    }

    py::array_t<double>  scale(py::array_t<double> A, py::array_t<double> B) { 
        py::array_t<double> result({10, 20});
        return result;
    }

};

PYBIND11_MODULE(genai, m) {
    m.doc() = "Example C++ module for Python";

    py::class_<DAG>(m, "DAG")
        .def(py::init<>())
        .def("addEdge", &DAG::addEdge)
        .def("printDAG", &DAG::printDAG);

    py::class_<Tensor>(m, "Tensor")
        .def(py::init<>())
        .def(py::init<const std::vector<double>&>())
        .def("addInput", &Tensor::addInput)
        .def("addOutput", &Tensor::addOutput)
        .def("forward", &Tensor::forward)
        .def("printData", &Tensor::printData);

    py::enum_<NodeType>(m, "NodeType")
        .value("Input", NodeType::Input)
        .value("Hidden", NodeType::Hidden)
        .value("Output", NodeType::Output)
        .export_values();

    py::class_<Node>(m, "Node")
        .def(py::init<const std::string&, NodeType>())
        .def("setInitialValues", &Node::setInitialValues)
        .def("forward", &Node::forward)
        .def("backward", &Node::backward)
        .def("printValues", &Node::printValues)
        .def("printGradients", &Node::printGradients);

    py::class_<Graph>(m, "Graph")
        .def(py::init<>())
        .def("connect", (void (Graph::*)(Node*,Node*)) &Graph::connect, "Connects this node to another node")
        .def("connect", (void (Graph::*)(std::vector<Node*>, Node*)) &Graph::connect, "Connects this node to multiple nodes")
        // .def("connect", [](Graph& graph, me, NodeType type) {
        //    return graph.createNode(name, type);
        // })
        // .def("connect", [](Graph& graph, const std::string& name, NodeType type) {
        //     return graph.createNode(name, type);
        // })
        .def("forwardPropagation", &Graph::forwardPropagation)
        .def("backwardPropagation", &Graph::backwardPropagation);

    py::class_<Graph>(m, "Graph")
        .def(py::init<>())
        .def("addNode", [](Graph& graph, const std::string& name, NodeType type, const std::vector<double>& initialValues) {
            return graph.createNode(name, type, initialValues);
        })
        .def("addNode", [](Graph& graph, const std::string& name, NodeType type) {
            return graph.createNode(name, type);
        })
        .def("connect", &Graph::connect)
        .def("forwardPropagation", &Graph::forwardPropagation)
        .def("backwardPropagation", &Graph::backwardPropagation);

    py::class_<Operator>(m, "Operator")
        .def(py::init<>())
        .def("matmul", &Operator::matmul)
        .def("linear", &Operator::linear)
        .def("normalize", &Operator::normalize)
        .def("mask", &Operator::mask)
        .def("softmax", &Operator::softmax)
        .def("scale", &Operator::scale);

    
    // Define function to print hello
    m.def("print_string", &print_string, "Print 'string'");
    m.def("print_double", &print_double, "Print 'double'");
    m.def("process_array", &process_array, "Process a NumPy array");
    m.def("process_matrix", &process_matrix, "Process a NumPy array");
    m.def("matmul", &matmul, "Matrix Multiplication a NumPy array");

}
