/******************************************************************************************************************************************
* Design considerations:
* 
* Designing an ideal structure for a neural network in C++ depends on various factors, including the specific requirements, 
* complexity of the network, and the desired level of flexibility. However, here are some key components and considerations 
* for building an efficient and modular neural network structure in C++:
* 
* Layer Abstraction: Define a base layer class that represents a generic layer in the neural network. This class should have 
* common methods like forward, backward, and updateWeights for performing forward propagation, backpropagation, and weight updates.
*
* Different Layer Implementations: Implement various types of layers as derived classes, such as fully connected layers, convolutional 
* layers, recurrent layers, activation layers, etc. Each layer implementation should inherit from the base layer class and provide 
* specific implementations for the required methods.
*
* Network Architecture: Define a network class that represents the overall neural network structure. It should encapsulate multiple 
* layers and provide methods for adding layers, connecting them, and performing forward and backward passes.
*
* Input and Output Handling: Consider how input data will be fed to the network and how output predictions will be obtained. Design 
* appropriate interfaces for loading and preprocessing input data and handling the output predictions.
*
* Training and Optimization: Implement training algorithms like stochastic gradient descent (SGD), mini-batch gradient descent, or more 
* advanced optimization methods. Include methods for setting up training parameters, initializing weights, and updating them during training.
* 
* Serialization and Persistence: Provide methods for saving and loading trained models to disk, allowing you to reuse or deploy the trained 
* network without retraining.
* 
* Integration with Libraries: Utilize existing C++ libraries for efficient numerical computations, such as BLAS libraries for matrix 
* operations or CUDA for GPU acceleration. Incorporate these libraries into your network implementation to improve performance.
* 
* Modularity and Flexibility: Design the structure in a modular and flexible way, allowing easy extension and customization. Consider 
* incorporating design patterns like Factory, Builder, or Strategy to enhance flexibility and reusability.
*
* System Design:
* Model Parallelism: Given the massive size of the model, it might be necessary to partition the parameters across multiple GPUs or devices 
* to enable efficient training. Model parallelism techniques, such as pipeline parallelism or model slicing, could be employed.
* 
* Distributed Training: Training a model with 175 billion parameters would likely require distributing the workload across multiple machines. 
* Distributed training frameworks like TensorFlow's Distributed Strategy or PyTorch's DistributedDataParallel can help parallelize the computations 
* and synchronize gradients across machines.
* 
* Cluster Infrastructure: A large-scale model would require a powerful cluster infrastructure with high-performance GPUs or specialized hardware 
* like TPUs. The number of machines and GPUs would depend on the specific requirements and scalability needs.
*
* Data Pipeline and Preprocessing: Handling large datasets efficiently is crucial. Designing a robust and scalable data pipeline that can 
* preprocess and load data in parallel is essential for training such models. Techniques like distributed data loading and data sharding can be employed.
*
* Model Architecture: The specific architecture of the model would depend on the task it aims to solve. For natural language processing tasks, 
* architectures like transformers have been successful. However, with 175 billion parameters, the architecture might involve complex variations, 
* deep hierarchies, and advanced attention mechanisms.
*
* Parameter Server or All-Reduce Approach: Coordinating the model parameters across different devices or machines can be done using a parameter 
* server architecture or an all-reduce approach. Both approaches have their trade-offs in terms of communication overhead and synchronization efficiency.
*
* Deployment Considerations: Deploying a large-scale model requires careful engineering and optimization to ensure efficient inference. 
* Techniques like model pruning, quantization, or specialized hardware deployment (e.g., using TensorRT or ONNX Runtime) might be considered to 
* improve inference speed and resource utilization.
*
* Monitoring and Scalability: Monitoring the training process, tracking model performance, and ensuring scalability are critical. Tools like 
* TensorFlow Extended (TFX) or Kubeflow can help with managing large-scale training pipelines and monitoring system metrics.
* 
* It's important to note that the above considerations provide a broad overview and may vary depending on the specific requirements, constraints, 
* and resources available. Designing a system for a model with 175 billion parameters is a highly complex task that requires deep expertise in 
* distributed systems, machine learning, and infrastructure design.
* 
* Attention Mechanism Considerations:
* Advanced attention mechanisms refer to more sophisticated and enhanced variations of the traditional attention mechanism. These variations 
* aim to address specific challenges or improve the effectiveness of attention-based models in different tasks. Here are a few examples:
* 
* Multi-head Attention: Instead of using a single attention mechanism, multi-head attention employs multiple parallel attention mechanisms 
* operating on different subspaces of the input. This allows the model to capture different types of information and learn more diverse representations.
*
* Self-Attention with Masking: In certain tasks, such as machine translation or language generation, it is essential to mask certain 
* positions in the input sequence to prevent the model from attending to future or unseen information. Masked self-attention ensures that 
* attention is only applied to valid positions in the sequence, taking into account the autoregressive nature of the task.
* 
* Relative Positional Encodings: Positional encodings are often used in attention mechanisms to incorporate positional information into the model. 
* Advanced attention mechanisms introduce relative positional encodings that capture relative distances or relationships between positions, 
* enabling the model to better understand the sequential structure of the input.
*
* Sparse Attention: In models with a large number of parameters, computing attention weights for all possible pairwise interactions can be 
* computationally expensive. Sparse attention mechanisms aim to reduce this computational burden by approximating the attention weights only for 
* a subset of the elements, typically based on their proximity or relevance to each other.
*
* Structured Attention: Traditional attention mechanisms operate on sequential or tabular data. However, in some domains, such as graphs or 
* images, attention needs to be applied to structured data. Advanced attention mechanisms adapt the attention mechanism to incorporate the 
* structural properties of the data, enabling the model to capture dependencies and relationships in a structured manner.
*
* Parameter server Approach (Distributed Training Strategies)
* Parameter server and all-reduce are two commonly used distributed training strategies in deep learning. Here's a comparison between 
* the two approaches:
* 
* Communication Pattern:
* 
* Parameter Server: In the parameter server approach, the model parameters are divided and stored on separate parameter servers. During 
* training, workers (e.g., GPUs or machines) communicate with the parameter servers to read and update the parameters.
* All-reduce: In the all-reduce approach, all workers participate in collective communication operations to synchronize their model parameters. 
* Each worker computes gradients locally, and all workers collectively reduce and update the gradients to ensure consistency across all replicas.
* Communication Overhead:
*
* Parameter Server: The parameter server approach involves communication between workers and parameter servers during parameter read and update 
* operations. The frequency of communication can be higher compared to all-reduce, especially when there are a large number of parameter servers 
* or when parameter updates are frequent.
* All-reduce: All-reduce involves communication among all workers during the gradient reduction step. The frequency of communication is 
* typically lower compared to parameter server, as workers exchange gradients periodically rather than after each update.
* Scalability:
* 
* Parameter Server: Parameter server architectures can scale to a large number of workers, as each worker communicates with a subset of parameter 
* servers. However, the scalability is limited by the communication bandwidth and latency between workers and parameter servers.
* All-reduce: All-reduce can scale efficiently to a large number of workers, as all workers participate in the collective communication. 
* The scalability of all-reduce depends on the network topology and the efficiency of the collective communication implementation.
* Fault Tolerance:

* Parameter Server: Parameter server architectures may suffer from single points of failure if a parameter server becomes unavailable. 
* Fault tolerance can be achieved by replicating the parameter servers or implementing backup mechanisms.
* All-reduce: All-reduce is inherently fault-tolerant as it relies on collective communication among all workers. If a worker fails, the 
* remaining workers can continue the training process.
* Memory and Storage Requirements:
* 
* Parameter Server: Parameter server architectures require storage for the model parameters on the parameter servers. The storage requirements 
* depend on the size of the model and the number of parameter servers.
* All-reduce: All-reduce requires memory for storing gradients during the reduction step. The memory requirements depend on the size of the 
* model and the number of workers.
* Both parameter server and all-reduce approaches have their strengths and weaknesses, and the choice depends on various factors such as the 
* size of the model, the number of workers, communication overhead, and fault tolerance requirements. In recent years, all-reduce has gained 
* popularity due to its scalability, fault tolerance, and efficient utilization of resources in distributed deep learning training.
*
* In terms of All-reduce approach:
* All-reduce algorithms, such as ring-based or tree-based algorithms, have demonstrated good scalability and efficiency in synchronous 
* gradient aggregation, reducing the communication overhead compared to parameter server architectures. This has made all-reduce more 
* attractive for large-scale distributed training, especially in scenarios with a large number of workers or when training large models.
* 
* However, it's important to note that the field of deep learning and distributed training is rapidly evolving. New techniques, frameworks, 
* and approaches continue to emerge, and the choice of distributed training strategy may vary depending on the specific requirements and 
* constraints of the training task.
* 
* To have the most up-to-date information on the current trends and practices in distributed deep learning training, I would recommend 
* referring to recent research papers, industry practices, and consulting with experts in the field.
*
* Ring-based and tree-based algorithms are two common approaches used in distributed computing, including in the context of all-reduce 
* operations in distributed deep learning. Here's a brief comparison of the two:
*
* Ring-based Algorithm:
*
* In a ring-based algorithm, the workers are arranged in a logical ring structure.
* The data is passed sequentially from one worker to the next in a circular manner until it reaches all the workers.
* Each worker performs a local reduction operation on the data it receives and then forwards the result to the next worker.
* The process continues until the data has been reduced by all workers and returned to the original sender.
* Ring-based algorithms are relatively simple to implement and have low latency but may suffer from load imbalance if the computation or communication 
* times vary significantly between workers.
*
* Tree-based Algorithm:
* 
* In a tree-based algorithm, the workers are organized in a hierarchical tree structure.
* The data is aggregated in a hierarchical manner, starting from the leaf nodes and moving up to the root node.
* Each node in the tree combines the data from its child nodes and performs a reduction operation.
* The process continues recursively until the root node receives the final reduced data.
* Tree-based algorithms can provide better load balancing compared to ring-based algorithms as the data aggregation happens in a 
* hierarchical structure.
* However, they may introduce higher latency due to additional communication steps involved in traversing the tree structure.
* The choice between ring-based and tree-based algorithms depends on various factors, such as the number of workers, the communication infrastructure, 
* and the characteristics of the training workload. Both algorithms have their strengths and weaknesses, and their performance can vary based on 
* the specific system and workload conditions.
* 
* It's worth noting that there are also other variations and optimizations of all-reduce algorithms, such as recursive doubling, butterfly, and more, 
* which aim to improve performance in different contexts. The choice of the most suitable algorithm often requires experimentation and benchmarking 
* on the target system to find the optimal configuration for a given distributed training task.
* 
* Here is a list of some commonly used variations and optimizations of all-reduce algorithms:
* 
* Ring-Based Algorithms: Traditional ring-based algorithms are widely used and serve as the baseline for many other algorithms.
* 
* Tree-Based Algorithms: Tree-based algorithms, such as binomial tree, k-ary tree, or hypercube-based tree, provide better load balancing and 
* reduced communication steps compared to ring-based algorithms.
* 
* Recursive Doubling: Recursive doubling algorithms leverage the binary representation of the rank to perform reduction operations in a hierarchical 
* manner, effectively reducing the number of communication steps.
* 
* Butterfly Algorithm: The butterfly algorithm uses a combination of butterfly networks and hypercube networks to achieve reduced latency and 
* improved bandwidth utilization.
* 
* AllGather: AllGather is an extension of all-reduce that collects the input data from all workers onto every worker, rather than performing a 
* reduction operation. It is commonly used for gathering statistics or exchanging information across all workers.
* 
* AllReduce-Multi: AllReduce-Multi algorithms allow simultaneous communication of multiple smaller messages instead of a single large message, which 
* can improve performance in certain scenarios, especially when dealing with heterogeneous network environments.
* 
* Gradient Compression: Gradient compression techniques, such as top-K sparsification or quantization, can be applied to reduce the communication 
* bandwidth and latency during the all-reduce operation while still maintaining reasonable model accuracy.
* 
* Ring All-Reduce with All-Gather: This approach combines the ring-based all-reduce with an all-gather operation to reduce the overall communication 
* time, especially when the number of workers is large.
*
* Gradient Accumulation: Gradient accumulation techniques allow workers to accumulate gradients over multiple iterations before performing the 
* all-reduce operation, reducing the frequency of communication and potentially improving scalability.
* 
* Asynchronous All-Reduce: Asynchronous algorithms, such as asynchronous decentralized parallel stochastic gradient descent (A-DePSGD), relax 
* the synchronization requirements and overlap communication with computation to improve overall training throughput.
* 
* These are just a few examples of the variations and optimizations available for all-reduce algorithms. The choice of which algorithm to use 
* depends on factors such as network topology, system characteristics, workload, and communication patterns, and it often requires careful 
* experimentation and benchmarking to identify the best approach for a specific distributed training scenario.
*
* In terms of Training distribution:
* Distributing training across multiple workers in a C++ codebase involves partitioning the data and model parameters, performing computations 
* on each worker, and synchronizing the updates to keep the model consistent. Here's a high-level overview of how training can be broken 
* down and distributed across workers:
* 
* Data Partitioning: Split the training data into multiple shards or subsets, where each worker is responsible for processing a different portion 
* of the data. The data can be partitioned based on samples, batches, or other appropriate criteria.
* 
* Model Replication: Replicate the model parameters on each worker. This ensures that each worker has a copy of the complete model for performing 
* computations independently.
* 
* Forward Pass: Each worker performs a forward pass on its local data subset using the replicated model. The forward pass computes the predictions 
* and loss for the local data.
* 
* Backward Pass and Gradient Computation: After the forward pass, each worker computes the gradients of the model parameters with respect to the 
* local data subset. The gradients can be computed using techniques like backpropagation or automatic differentiation.
* 
* Gradient Aggregation: The computed gradients from each worker need to be aggregated to obtain a global gradient. This can be done using various 
* aggregation algorithms, such as the All-Reduce algorithm, where gradients are exchanged and combined across workers to compute the average or sum 
* of gradients.
* 
* Parameter Update: Once the global gradient is obtained, each worker updates its local copy of the model parameters using an optimization algorithm, 
* such as stochastic gradient descent (SGD) or Adam. The updates can be applied asynchronously or synchronously based on the distributed training strategy.
* 
* Synchronization: If training is performed asynchronously, it is necessary to periodically synchronize the model parameters across workers to maintain 
* consistency. Synchronization can be done by broadcasting the updated model parameters from a designated worker to other workers.
*
* Iterative Training: The above steps are repeated for multiple iterations or epochs until the desired convergence or training criteria are met. Each 
* iteration involves data partitioning, forward and backward passes, gradient aggregation, parameter updates, and synchronization.
*
* It's important to note that the implementation details of distributed training in C++ may vary depending on the specific framework or library being 
* used. Popular frameworks like TensorFlow or PyTorch provide built-in support for distributed training with their own APIs and abstractions. 
* These frameworks handle the underlying communication, synchronization, and parameter updates across workers, allowing you to focus more on 
* defining the model and training process.
*
* When implementing distributed training in C++, you may need to utilize distributed computing libraries, such as MPI (Message Passing Interface) or 
* specialized distributed frameworks like Horovod, to facilitate inter-worker communication and coordination. These libraries provide functions and 
* utilities for message passing, collective operations, and distributed training patterns.
* 
* Overall, the process of distributing training across workers in C++ involves partitioning data, replicating the model parameters, performing 
* computations on each worker, aggregating gradients, updating parameters, and ensuring synchronization to achieve distributed training and collaboration.
******************************************************************************************************************************************/
#include <iostream>
#include <limits>
#include <unordered_map>
#include <unordered_set>
#include <vector>
#include <string>
#include <cmath>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <pybind11/embed.h>
#include <pybind11/numpy.h>
#include <cblas.h>
#include <omp.h>
#include <mpi.h>
#include <Eigen/Dense>
#include <Eigen/Core>
#include <memory>
#include <queue>
#include <thread>
#include <mutex>
#include <ctime>
#include <chrono>

namespace py = pybind11;
using namespace py::literals;


void log_msg(const std::string& text) {
    std::cout << text << std::endl;
}

double inf() {
    return std::numeric_limits<double>::infinity();
}

void print_string(const std::string& text, bool printNextLine) {
    if (printNextLine) {
        py::print(text);
    } else {
        py::print(text, py::arg("end") = "");
    }
}

void print_double(double value, bool printNextLine) {
    if (printNextLine) {
        py::print(value);
    } else {
        py::print(value, py::arg("end") = "");
    }
}

double* allocate_matrix(ssize_t rows, ssize_t cols) {
    // Allocate memory for the matrix
    double* matrix = (double*)malloc(rows * cols * sizeof(double));
    return matrix;
}

void process_array1(py::array_t<double> inputArray) {
    // Access the underlying NumPy array data
    py::buffer_info bufInfo = inputArray.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    // Access the shape and strides of the array
    std::vector<size_t> shape(bufInfo.shape.begin(), bufInfo.shape.end());
    std::vector<size_t> strides(bufInfo.strides.begin(), bufInfo.strides.end());

    // Iterate over the array elements
    for (size_t i = 0; i < shape[0]; ++i) {
        for (size_t j = 0; j < shape[1]; ++j) {
            // Access the element at (i, j)
            double value = dataPtr[i * strides[0] + j * strides[1]];
            // Process the element as needed
            // ...
            print_double(value, true);
        }
    }
}

void process_array(py::array_t<double> inputArray) {
    py::print("Processing array:");

/*
    auto bufInfo = inputArray.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    ssize_t size = bufInfo.size;

    for (ssize_t i = 0; i < size; ++i) {
        double value = dataPtr[i];
        py::print(value);
    }
*/

    py::buffer_info bufInfo = inputArray.request();

    // Get the shape of the array
    std::vector<size_t> shape(bufInfo.shape.begin(), bufInfo.shape.end());

    // Get the shape of the array
    // std::vector<size_t> shape = bufInfo.shape;

    // Determine the number of dimensions
    size_t numDimensions = shape.size();

    // Print the dimensions
    py::print("Number of dimensions:", numDimensions);
    py::print("Shape:", shape);

}

void process_matrix(py::array_t<double> inputMatrix) {
    py::print("Processing matrix:");

    auto bufInfo = inputMatrix.request();
    double* dataPtr = static_cast<double*>(bufInfo.ptr);

    ssize_t rows = bufInfo.shape[0];
    ssize_t cols = bufInfo.shape[1];

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = dataPtr[i * cols + j];
            // py::print(value);
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }
}

py::array_t<double>  matmul(py::array_t<double> A, py::array_t<double> B) {
    py::print("Processing matrix:");

    auto bufInfoA = A.request();
    double* matA = static_cast<double*>(bufInfoA.ptr);

    auto bufInfoB = B.request();
    double* matB = static_cast<double*>(bufInfoB.ptr);

    ssize_t rows = bufInfoA.shape[0];
    ssize_t cols = bufInfoA.shape[1];

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = matA[i * cols + j];
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }

    for (ssize_t i = 0; i < rows; ++i) {
        for (ssize_t j = 0; j < cols; ++j) {
            double value = matB[i * cols + j];
            print_double(value, false);
            print_string(" ", false);
        }
        print_string("", true);
    }

    int rows_a = rows;
    int cols_a = cols;
    int cols_b = cols;

    // Create a new NumPy array with the same shape as the matrix
    // This actually allocates memory.
    py::array_t<double> result({rows, cols});

    // Get a pointer to the underlying data of the NumPy array
    double* matC = result.mutable_data();

    // otherwise, allocate manually
    // double* matC = allocate_matrix(rows, cols);

    float alpha = 1.0;
    float beta  = 0.0;
    cblas_dgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, rows_a, cols_b, cols_a, alpha,
                (double *) matA, cols_a,
                (double *) matB, cols_b, beta,
                (double *) matC, cols_b);

    return result;
}


struct OperationParams {
    Eigen::MatrixXd weights; // MxW
    Eigen::RowVectorXd biases;  // 1xW
};

// Create the MPI data type for std::vector<OperationParams>
MPI_Datatype OperationParamsMPIType;
MPI_Datatype VectorOperationParamsMPIType;

// Create the MPI data type for std::vector<OperationParams>
void  createVectorOperationParamsMPIType() {

    // Create an MPI datatype for the OperationParams struct
    int blockcounts[2] = {1, 1};
    MPI_Datatype types[2] = {MPI_DOUBLE, MPI_DOUBLE};
    MPI_Aint offsets[2];

    offsets[0] = offsetof(OperationParams, weights);
    offsets[1] = offsetof(OperationParams, biases);

    MPI_Type_create_struct(2, blockcounts, offsets, types, &OperationParamsMPIType);
    MPI_Type_commit(&OperationParamsMPIType);

    // Create an MPI datatype for std::vector<OperationParams>
    int vectorBlockcounts[1] = {0};
    MPI_Aint vectorOffsets[1] = {0};
    MPI_Datatype vectorTypes[1] = {OperationParamsMPIType};

    MPI_Type_create_struct(1, vectorBlockcounts, vectorOffsets, vectorTypes, &VectorOperationParamsMPIType);
    MPI_Type_commit(&VectorOperationParamsMPIType);

}

// Free the MPI data type
void freeVectorOperationParamsMPIType() {
    MPI_Type_free(&VectorOperationParamsMPIType);
    MPI_Type_free(&OperationParamsMPIType);
}


MPI_Datatype MatrixXdMPIType;
MPI_Datatype VectorXdMPIType;

void  createMPITypes() {
    // Create MPI data type for Eigen's MatrixXd
    MPI_Type_contiguous(Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>::RowsAtCompileTime *
                        Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>::ColsAtCompileTime,
                        MPI_DOUBLE, &MatrixXdMPIType);
    MPI_Type_commit(&MatrixXdMPIType);

    // Create MPI data type for Eigen's VectorXd
    MPI_Type_contiguous(Eigen::Matrix<double, Eigen::Dynamic, 1>::RowsAtCompileTime,
                        MPI_DOUBLE, &VectorXdMPIType);
    MPI_Type_commit(&VectorXdMPIType);
}

// Free the MPI data type
void freeMPITypes() {
    MPI_Type_free(&MatrixXdMPIType);
    MPI_Type_free(&VectorXdMPIType);
}

enum class OperType {
    LINEAR,
    BATCHNORM,
    LAYERNORM,
    REDUCT,
    ACTIVATE,
    MASK,
    DROPOUT,
    SCALE,
    ATTENTION
};

enum class ReductionType {
    SUM,
    AVG,
    MAX,
    MIN,
    ARGMAX,
    ARGMIN,
    MATMUL,
    MUL
};

enum class ActivationType {
    SIGMOID,
    TANH,
    RELU,
    LEAKYRELU,
    GELU,
    SOFTMAX
};

/*****************************************************************************************************
* Base Operators
*  Linear, BatchNorm, LayerNorm, Reduct classes derive from BaseOperator class.
*****************************************************************************************************/
class BaseOperator {
    private:
        OperType otype;
    public:
    virtual void forwardPass() = 0;
    virtual void backwardPass() = 0;

    // Perform Matrix Multiplication. 
    static Eigen::MatrixXd matmul(Eigen::MatrixXd A, Eigen::MatrixXd B) {
        double alpha = 1.0;
        double beta = 0.0;
        int M = A.rows();
        int N = B.cols();
        int K = A.cols();
        int lda = M;  // leading dimension of A.
        int ldb = K;  // leading dimension of B.
        int ldc = M;  // leading dimension of C.

        Eigen::MatrixXd C(M, N);

        // Default:
        // Here we assume the following:
        //  A = MxK,  B = KxN,    C = MxN. 
        // Therefore for a Column Major (Vertical), lda = M, ldb = K, ldc = M. Those represent the length of rows per matrix.
        cblas_dgemm(CblasColMajor, CblasNoTrans, CblasNoTrans, M, N, K, alpha, A.data(), lda, B.data(), ldb, beta, C.data(), ldc);

        return C;
    }

    static Eigen::MatrixXd softmax(const Eigen::MatrixXd& input_data) {

        std::cout << "Softmax forward ...\n";
        std::cout << input_data << "\n";

        // Find the maximum value in each column of x
        // Required to handle large numbers.
        double maxVal = input_data.maxCoeff();

        std::cout << "maxVal maxCoeff forward ...\n";
        std::cout << maxVal << "\n";

        // Subtract the maximum value from each element in each column of x and compute the exponential
        Eigen::MatrixXd expX = (input_data.array().colwise() - Eigen::ArrayXd::Constant(input_data.cols(), maxVal)).exp();

        std::cout << "expX forward ...\n";
        std::cout << expX << "\n";

        // Compute the sum of exponential values for each row
        Eigen::VectorXd sumExp = expX.rowwise().sum();

        std::cout << "sumExp forward ...\n";
        std::cout << sumExp << "\n";

        // Compute the softmax probabilities by dividing each element in each row by the sum of exponential values
        Eigen::MatrixXd softmax = expX.array() / sumExp.replicate(1,2).array();

        std::cout << "softmax forward ...\n";
        std::cout << softmax << "\n";

        // Return the computed softmax probabilities 
        return softmax;
    }

    /****************************************************************************************************
     * The gradient of the softmax function with respect to its input (z) can be expressed as follows:
     * dy_i/dz_j = propagated_gradient * (softmax(z_i) * (1 - softmax(z_j))) for i = j
     * dy_i/dz_j = propagated_gradient * (-softmax(z_i) * softmax(z_j)) for i != j
     * Or if output (y) is cached, where y = softmax(z)
     * then we use the output such that:
     * dy_i/dz_j = propagated_gradient * (y_i * (1 - y_j)) for i = j
     * dy_i/dz_j = propagated_gradient * (-y_i * y_j) for i != j
     ****************************************************************************************************/
    Eigen::MatrixXd softmaxGradient(const Eigen::MatrixXd& gradients, const Eigen::MatrixXd& output_data) {
        int N = gradients.rows();  // Number of samples
        int M = gradients.cols();  // Number of classes

        Eigen::MatrixXd dInput(N, M);
        dInput.setZero();

        std::cout << "Softmax Gradient \n";
        std::cout << "N:" << N << "\n";
        std::cout << "M:" << M << "\n";

        std::cout << gradients << "\n";

        for (int i = 0; i < N; ++i) {
            for (int j = 0; j < M; ++j) {
                for (int k = 0; k < M; ++k) {
                    if (k == j) {
                        dInput(i, j) += gradients(i, k) * output_data(i, j) * (1 - output_data(i, j));
                    } else {
                        dInput(i, j) -= gradients(i, k) * output_data(i, j) * output_data(i, k);
                    }
                }
            }
        }
        return dInput;
    }

    static void xavierInitialization(Eigen::MatrixXd& weights) {
        double scale = std::sqrt(6.0 / (weights.rows() + weights.cols()));
        weights.setRandom();
        weights *= scale;
    }

    static void xavierInitialization(Eigen::VectorXd& biases) {
        double scale = std::sqrt(6.0 / (biases.cols() + biases.rows()));
        biases.setRandom();
        biases *= scale;
    }

    static void xavierInitialization(Eigen::RowVectorXd& biases) {
        double scale = std::sqrt(6.0 / (biases.cols() + biases.rows()));
        biases.setRandom();
        biases *= scale;
    }

    static void heInitialization(Eigen::MatrixXd& weights) {
        double scale = std::sqrt(2.0 / weights.rows());
        weights.setRandom();
        weights *= scale;
    }

    static void heInitialization(Eigen::VectorXd& biases) {
        double scale = std::sqrt(2.0 / biases.rows());
        biases.setRandom();
        biases *= scale;
    }

    static void heInitialization(Eigen::RowVectorXd& biases) {
        double scale = std::sqrt(2.0 / biases.cols());
        biases.setRandom();
        biases *= scale;
    }

    static void uniformInitialization(Eigen::MatrixXd& weights, double minVal, double maxVal) {
        weights = Eigen::MatrixXd::Random(weights.rows(), weights.cols());
        weights = (weights.array() * (maxVal - minVal)) + minVal;
    }

    static void normalInitialization(Eigen::MatrixXd& weights, double mean, double stdDev) {
        weights = Eigen::MatrixXd::Random(weights.rows(), weights.cols());
        weights = (weights.array() * stdDev) + mean;
    }

    static void zeroInitialization(Eigen::MatrixXd& weights) {
        weights.setZero();
    }



};

/*****************************************************************************************************
* Some common reduction operations used in machine learning and deep learning include:
*   Sum: Computes the sum of all the values.
*   Avg: Computes the average of all the values.
*   Max: Finds the maximum value among all the values.
*   Min: Finds the minimum value among all the values.
*   Argmax: Returns the index or position of the maximum value.
*   Argmin: Returns the index or position of the minimum value.
*   Matmul: Returns matrix multiplication result.
*   Mul: Returns element-wise multiplication result.
*****************************************************************************************************/
class Reduction : public BaseOperator {
private:
    Eigen::MatrixXd input_data;
    std::string reducttype = "add";

public:
    Reduction(const std::string& reducttype = "add") {
        this->reducttype = reducttype;
    }

    std::string getType() {
        return this->reducttype;
    }

    void forwardPass() { print_string("Reduct forward pass ...", true); }
    void backwardPass() {}
};
/*****************************************************************************************************
* Base Optimizer Functions
*****************************************************************************************************/
class Optimizer : public BaseOperator {
private:
    std::string optimizertype = "adam";
    double learningRate = 0.001;
    Eigen::MatrixXd moments;
    Eigen::MatrixXd velocity;
    Eigen::MatrixXd rho;
    Eigen::MatrixXd rms;
    Eigen::MatrixXd accum;
    Eigen::MatrixXd nu;
public:

    Optimizer(const std::string& optimizertype, double& learningRate) {
        this->optimizertype = optimizertype;
        this->learningRate = learningRate; 
        moments.setZero();  
        velocity.setZero();  
        rho.setZero();   
        rms.setZero();   
        accum.setZero();  
        nu.setZero();  
    }

    // SGD optimizer with optional step decay
    void sgd(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0,
                    bool useStepDecay = false, double decayRateStep = 0.1, int decayStep = 0) {
        // Update weights
        weights.array() -= learningRate * gradients.array();

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // Momentum optimizer with optional step decay
    void momentum(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0,
                    double momentumRate = 0.9,
                    bool useStepDecay = false, double decayRateStep = 0.1,  int decayStep = 0) {
        // Initialize Momentum optimizer variables
        //static Eigen::MatrixXd velocity = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        if (velocity.cols() == 0 && velocity.rows() == 0) {
            velocity = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }

        // Update velocity
        velocity = momentumRate * velocity + learningRate * gradients;

        // Update weights
        weights.array() -= velocity.array();

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // Adam optimizer with optional step decay
    void adam(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0,
                    double beta1 = 0.9, double beta2 = 0.999, double epsilon = 1e-8,
                    bool useStepDecay = false, double decayRateStep = 0.1,  int decayStep = 0) {
        // Initialize Adam optimizer variables
        //static Eigen::MatrixXd m = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
       // static Eigen::MatrixXd v = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());

        if (moments.cols() == 0 && moments.rows() == 0) {
            moments = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }
        if (velocity.cols() == 0 && velocity.rows() == 0) {
            velocity = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }

        double beta1_t = std::pow(beta1, currentEpoch + 1);
        double beta2_t = std::pow(beta2, currentEpoch + 1);

        // Print the value with the specified precision
        float value1 = 1.23456789f;
        int floatPrecision = std::numeric_limits<float>::digits10;
        std::cout.precision(floatPrecision);
        std::cout << "Value: " << value1 << std::endl;

        // Print the value with the specified precision
        double value2 = 1.23456789;
        int doublePrecision = std::numeric_limits<double>::digits10;
        std::cout.precision(doublePrecision);
        std::cout << "Value: " << value2 << std::endl;

        std::cout << "beta1_t\n";
        std::cout << beta2_t << "\n";
        std::cout << "beta2_t\n";
        std::cout << beta2_t << "\n";

        std::cout << "beta1 * m\n";
        std::cout << beta1 * moments.array() << "\n";

        std::cout << "beta2 * v\n";
        std::cout << beta2 * velocity.array() << "\n";

        // Update momentum and velocity
        moments = beta1 * moments.array() + (1 - beta1) * gradients.array();
        velocity = beta2 * velocity.array() + (1 - beta2) * (gradients.array() * gradients.array());

        std::cout << "gradients\n";
        std::cout << gradients.array()  << "\n";

        std::cout << "power of gradients\n";
        std::cout << gradients.array() * gradients.array() << "\n";


        std::cout << "momentum\n";
        std::cout << moments << "\n";
        std::cout << "velocity\n";
        std::cout << velocity << "\n";

        // Compute bias-corrected moment estimates
        Eigen::MatrixXd m_hat = moments / (1 - beta1_t);
        Eigen::MatrixXd v_hat = velocity / (1 - beta2_t);

        std::cout << "momentum hat\n";
        std::cout << m_hat << "\n";
        std::cout << "velocity hat\n";
        std::cout << v_hat << "\n";

        // Update weights
        weights.array() -= learningRate * (m_hat.array() / (v_hat.array().sqrt() + epsilon));

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // RMSprop optimizer with optional step decay
    void rmsprop(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0,
                    double rho = 0.9, double epsilon = 1e-8,
                    bool useStepDecay = false, double decayRateStep = 0.1,  int decayStep = 0) {
        // Initialize RMSprop optimizer variables
        // static Eigen::MatrixXd rms = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        if (rms.cols() == 0 && rms.rows() == 0) {
            rms = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }

        // Update RMSprop cache
        rms = rho * rms + (1 - rho) * (gradients.array() * gradients.array()).matrix();

        // Update weights
        weights.array() -= learningRate * (gradients.array() / (rms.array().sqrt() + epsilon));

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // Adagrad optimizer with optional step decay
    void adagrad(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0,
                    double epsilon = 1e-8,
                    bool useStepDecay = false, double decayRateStep = 0.1,  int decayStep = 0) {
        // Initialize Adagrad optimizer variables
        // static Eigen::MatrixXd accum = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        if (accum.cols() == 0 && accum.rows() == 0) {
            accum = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }

        // Update sum of squared gradients
        accum.array() += gradients.array() * gradients.array();

        // Update weights
        weights.array() -= learningRate * gradients.array() / (accum.array().sqrt() + epsilon);

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // Adamax optimizer with optional step decay
    void adamax(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0, 
                    double beta1 = 0.9, double beta2 = 0.999, double epsilon = 1e-8,
                    bool useStepDecay = false, double decayRateStep = 0.1, int decayStep = 0) {
        // Initialize Adamax optimizer variables
        //static Eigen::MatrixXd m = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        //static Eigen::MatrixXd u = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        if (moments.cols() == 0 && moments.rows() == 0) {
            moments = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }
        if (nu.cols() == 0 && nu.rows() == 0) {
            nu = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }

        // Update biased first moment estimate
        moments = beta1 * moments + (1 - beta1) * gradients;

        // Update the exponentially weighted infinity norm
        nu = nu.cwiseMax(beta2 * nu.cwiseAbs()) + (1 - beta2) * gradients.cwiseAbs();

        // Update weights
        weights.array() -= learningRate * (moments.array() / (nu.array() + epsilon));

        // Compute learning rate decay
        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
        
    }

    // Nadam optimizer with optional step decay
    void nadam(Eigen::MatrixXd& weights, Eigen::MatrixXd& gradients, int currentEpoch = 0, 
                    double beta1 = 0.9, double beta2 = 0.999, double epsilon = 1e-8,
                    bool useStepDecay = false, double decayRateStep = 0.1, int decayStep = 0) {
        // Initialize Nadam optimizer variables
        //static Eigen::MatrixXd m = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        //static Eigen::MatrixXd v = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        if (moments.cols() == 0 && moments.rows() == 0) {
            moments = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }
        if (velocity.cols() == 0 && velocity.rows() == 0) {
            velocity = Eigen::MatrixXd::Zero(weights.rows(), weights.cols());
        }


        double beta1_t = std::pow(beta1, currentEpoch + 1);
        double beta2_t = std::pow(beta2, currentEpoch + 1);

        // Update momentum and velocity
        moments = beta1 * moments + (1 - beta1) * gradients;
        velocity = beta2 * velocity + (1 - beta2) * (gradients.array() * gradients.array()).matrix();

        // Compute bias-corrected moment estimates
        Eigen::MatrixXd m_hat = moments / (1 - beta1_t);
        Eigen::MatrixXd v_hat = velocity / (1 - beta2_t);

        // Update weights
        weights.array() -= learningRate * (beta1 * m_hat + (1 - beta1) * gradients).array()
                   / (v_hat.array().sqrt() + epsilon);

        if (useStepDecay) {
            stepDecay(learningRate, decayRateStep, currentEpoch, decayStep);
        }
    }

    // Step decay for learning rate
    void stepDecay(double& learningRate, double decayRate, int currentEpoch, int decayStep) {
        if (currentEpoch != 0 && currentEpoch % decayStep == 0) {
            learningRate *= decayRate;
        }
    }

    void forwardPass() { print_string("Optimizer forward pass ...", true); }
    void backwardPass() {}

};
/**************************************************************************************************************************
 * Linear Class:
***************************************************************************************************************************/
class Linear : public BaseOperator {
private:

    Eigen::MatrixXd input_data; // NxW samples, by using & 
    OperationParams parameters; // inputs to next forward-wise Nodes
    OperationParams gradients;  // inputs to next backward-wise Nodes   (gradients with respect to weights & biases)

    int M = 0; // number of features (embedding vector size)
    int W = 0; // number of weights 
    bool bias = true; // Use bias by default.

    Optimizer* opt_weights = nullptr; // for optimizer
    Optimizer* opt_biases = nullptr; // for optimizer

public:
    Linear(int size, bool bias = true)  {
        this->W = size;
        this->bias = bias;
        print_string("Linear operation ...", true);
    }

    // This assumes that the input is defined with NxM dimensionality.
    // Therefore the size of the parameters and thus gradients will be based on MxW where W is the number of weights to use;
    // The output will have NxW dimension.
    void setInitialWeights(int M) {

        // if size is already set, 
        // it means weights have previously already been set by an initial forward pass
        if (this->M != 0) return;

        this->M = M;
   
        parameters.weights.resize(M, W); // allocates memory
        parameters.biases.resize(W); // allocates memory

        // Initialize Weights & Biases
        heInitialization(parameters.weights);
        // heInitialization(parameters.biases);
        parameters.biases.setConstant(0.01);

        gradients.weights.resize(M, W); // allocates memory
        gradients.biases.resize(W); // allocates memory

        // Initialize Gradients     
        gradients.weights.setZero();
        gradients.biases.setZero();

    }

    OperationParams getParameters() const {
        return parameters;
    }

    OperationParams getGradients() const {
        return gradients;
    }

    // While the parameter weight has dimension MxW,  the resulting transformation has dimension of NxW.
    // We only need the M dimension from an NxM input to generate parameter matrix.
    // where weights is NxW and bias is W.
    Eigen::MatrixXd linearTransform(const Eigen::MatrixXd& input_data) {

        std::cout << "Entering Linear Transformation\n";

        // Initilalize the parameters.
        setInitialWeights(input_data.cols());
        Eigen::MatrixXd& weights = parameters.weights;
        Eigen::RowVectorXd& biases = parameters.biases;
        std::cout << "Size of input: " << input_data.size() << "\n";
        std::cout << "Size of weights: "  << weights.size() << "\n";
        std::cout << "Size of biases: " << biases.size() << "\n";

        std::cout << "xxxxx \n";
        std::cout << "the input ...\n";
        std::cout << input_data  << "\n\n";
        std::cout << "weights\n";
        std::cout << weights << "\n\n";
        std::cout << "matmuls 1\n";
        std::cout << input_data * weights << "\n\n";
        std::cout << "matmuls 2\n";
        std::cout << BaseOperator::matmul(input_data, weights) << "\n\n";
        std::cout << "bias\n";
        std::cout << biases << "\n";
        std::cout << "yyyyy \n";

        Eigen::MatrixXd output;
        
        if (bias == true) {
            // rowwise() means, slice one row at a time and add to bias (which is 1 row horizontally).
            output = BaseOperator::matmul(input_data, weights).rowwise() + biases; // Ax + b = Wx + b; NxW dimension.
        } else {
            output = BaseOperator::matmul(input_data, weights); // Ax   = Wx; NxW dimension.
        }
        return output;
    }

    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        std::cout << "Size of input:" << this->input_data.size() << "\n";

        // Perform Linear Transformation.
        Eigen::MatrixXd output = linearTransform(input_data);

        print_string("Linear forward pass ...", true); 

        return output; // this becomes input to the next Node or next Layer.
    }

    void gradient_Wrt_Weight_Bias(Eigen::MatrixXd& gradient) {
        int N = gradient.rows();
        
        std::cout << "Size of gradient:" << gradient.size() << "\n";
        std::cout << "Size of input:" << this->input_data.size() << "\n";
        std::cout << "Size of tranposed input:" << (this->input_data).size() << "\n";

        std::cout << "Before multiply of input and gradients\n";
        std::cout << "the input:\n";
        std::cout << this->input_data << "\n";
        std::cout << "the gradient:\n";
        std::cout << gradient << "\n";
        std::cout << "the initial gradient:\n";
        std::cout << gradients.weights << "\n";
        
        std::cout << "Multiply input and gradient.\n";
        // Compute the gradient with respect to the weights (dW)
        gradients.weights = BaseOperator::matmul(gradient.transpose(), this->input_data).transpose();  // dL/W = (dL/DC.T * x).T
        
        std::cout << "the input:\n";
        std::cout << this->input_data << "\n";
        std::cout << "the gradient:\n";
        std::cout << gradient << "\n";
        std::cout << "the computed gradient with respect to weight:\n";
        std::cout << gradients.weights << "\n";

        std::cout << "Add the bias.\n";
        // Compute the gradient with respect to the bias (db)
        if (bias == true)  gradients.biases = gradient.colwise().sum();

        std::cout << "gbiases rows: " <<  gradients.biases.rows() << ", cols: " <<  gradients.biases.cols() << "\n";
        std::cout << gradients.biases << "\n";
        
         std::cout << "Normalize the gradients.\n";
        // Normalize the gradients by dividing by the number of samples (N)
        gradients.weights /= N;  // dW - gradients of weights (MxW)
        if (bias == true) gradients.biases /= N;   // db - gradients of bias (1xW)
        std::cout << "Normalized gradients ...\n";
        std::cout << "gradient weights\n";
        std::cout << gradients.weights << "\n";
        std::cout << "gradient biases\n";
        std::cout << gradients.biases << "\n";
    }

    Eigen::MatrixXd gradient_Wrt_Input(Eigen::MatrixXd& gradient) { 
        int N = gradient.rows();
        std::cout << "the weight:\n";
        std::cout << parameters.weights << "\n";
        std::cout << "the gradient:\n";
        std::cout << gradient << "\n";
        // Compute the gradient with respect to the input (dInput)
        Eigen::MatrixXd dInput = BaseOperator::matmul(parameters.weights, gradient.transpose()).transpose();   // dL/x = (W * dL/DC.T)

        std::cout << "the computed gradient with respect to input:\n";
        std::cout << dInput << "\n";

        // Normalize the gradients by dividing by the number of samples (N)
        dInput /= N;  // dInput - gradients of input (NxM)

        std::cout << "normalized dInput\n";
        std::cout << dInput << "\n";
        return dInput;
    }

    // Leave the gradients as is. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(Eigen::MatrixXd& gradients) { 
        std::cout << "Entering Linear Gradient backward pass ...\n";
        Eigen::MatrixXd& weights = parameters.weights;
        Eigen::RowVectorXd& biases = parameters.biases;
        std::cout << "Size of gradients: " << gradients.size() << "\n";
        std::cout << "Size of weights: "  << weights.size() << "\n";
        std::cout << "Size of biases: " << biases.size() << "\n";
        std::cout << "Computing Gradient now ....\n";
        gradient_Wrt_Weight_Bias(gradients);
        std::cout << "Computing Delta Error now ....\n";
        Eigen::MatrixXd dInput = gradient_Wrt_Input(gradients);
        std::cout << "Done ...\n";
        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        std::cout << "LInear parameter updating in ...\n";
        std::cout << "size: " << this->W << " \n";
        Eigen::MatrixXd pbiases = parameters.biases.matrix();
        Eigen::MatrixXd gbiases = gradients.biases.matrix(); 
        std::cout << "Assigning ...\n";

        std::cout << "pbiases rows: " << pbiases.rows() << ", cols: " << pbiases.cols() << "\n";
        std::cout << "gbiases rows: " << gbiases.rows() << ", cols: " << gbiases.cols() << "\n";

        std::cout << "Gradient weights:\n";
        std::cout << gradients.weights << "\n";
        std::cout << "Gradient biases:\n";
        std::cout << gradients.biases << "\n";

        std::cout << "Before Updated weights:\n";
        std::cout << parameters.weights << "\n";
        std::cout << "Before Updated biases:\n";
        std::cout << parameters.biases << "\n";

        if (opt_weights == nullptr) {
            opt_weights = new Optimizer(optimizertype, learningRate);
        }

        if (opt_biases == nullptr && bias == true) {
            opt_biases = new Optimizer(optimizertype, learningRate);
        }

        std::cout << "Updating Linear weights  \n";
        opt_weights->adam(parameters.weights, gradients.weights, iter);
        std::cout << "Updating Linear biases \n";

        if (bias == true) {
            opt_biases->adam(pbiases, gbiases, iter);
            parameters.biases = pbiases.row(0);
        }

        std::cout << "Updated weights:\n";
        std::cout << parameters.weights << "\n";
        std::cout << "Updated biases:\n";
        std::cout << parameters.biases << "\n";

        // initialize gradients for next iteration.
        gradients.weights.setZero();
        gradients.biases.setZero();
    }

    void forwardPass() { print_string("Linear forward pass ...", true); }
    void backwardPass() {}
    
};

// Suppose we have a sample input represented as a matrix of NxM where N=number of samples
// and M=embedding vector size (features).  Our Batch normalization takes mean and variance
// along the N dimension.
/*****************************************************************************************************
* Base Batch Normalization Function
*****************************************************************************************************/
class BatchNorm : public BaseOperator {
private:
    // Across W dimension
    Eigen::RowVectorXd scale; // (gamma) 
    Eigen::RowVectorXd shift; // (beta)
    Eigen::RowVectorXd dScale; // gradient for scale (gamma)
    Eigen::RowVectorXd dShift; // gradient for the shift (beta)

    // Cached data for backpropagation.
    Eigen::MatrixXd input_data; // NxW samples 
    Eigen::MatrixXd normalizedInput; // NxW samples 
    Eigen::MatrixXd minusMean;
    Eigen::RowVectorXd batchStdDev;
    int M = 0;

    Optimizer* opt_scale = nullptr; // for optimizer
    Optimizer* opt_shift = nullptr; // for optimizer

public:
    BatchNorm(int size) {
      // initialize gradients for next iteration.
        dScale.setZero();
        dShift.setZero();
    }

    // This assumes that the input is defined with NxM dimensionality.
    // Therefore the size of the parameters and thus gradients will be based on MxW where W is the number of weights to use.
    void setInitialWeights(int M) {

        // if size is already set, 
        // it means weights have previously already been set by an initial forward pass
        if (this->M != 0) return;

        this->M = M;

        // Initialize scaling and shifting parameters   
        scale.resize(this->M); // allocates memory
        shift.resize(this->M); // allocates memory

        // Initialize Weights & Biases
        heInitialization(scale);
        //heInitialization(shift);
        shift.setConstant(0.01);

        // Initialize Gradients     
        dScale.setZero();
        dShift.setZero();
    }

    Eigen::MatrixXd normalize(const Eigen::MatrixXd& input_data) {

        setInitialWeights(input_data.cols());

        std::cout << input_data << std::endl;

        // Calculate batch mean along the N dimension, but along the M dimension.
        Eigen::RowVectorXd batchMean = input_data.colwise().mean();

        std::cout << "Mean ..." << std::endl;
        std::cout << batchMean << std::endl;

        // Calculate: X - mean
        minusMean = input_data.rowwise() - batchMean;

        std::cout << "minusMean\n";
        std::cout << minusMean << "\n";

        // Calculate batch variance along the N dimension
        Eigen::RowVectorXd batchVariance = minusMean.array().square().colwise().mean();

        std::cout << "Variance ..." << std::endl;
        std::cout << batchVariance << std::endl;

        // Add a small epsilon for numerical stability
        double epsilon = 1e-8;
        Eigen::RowVectorXd epsilonVector = Eigen::RowVectorXd::Constant(batchVariance.size(), epsilon);

        std::cout << "Epsilon ..." << std::endl;
        std::cout << epsilonVector << std::endl;

        // Calculate batch standard deviation along the N dimension
        batchStdDev = (batchVariance + epsilonVector).cwiseSqrt();

        std::cout << "stdDev ..." << std::endl;
        std::cout << batchStdDev << std::endl;

        // Normalize the inputs along the N dimension
        normalizedInput = minusMean.array().rowwise()  / batchStdDev.array();

        std::cout << "normalizedInput along the N  ..." << std::endl;
        std::cout << normalizedInput << std::endl;

        // Scale and shift the normalized inputs along the N dimension.
        Eigen::MatrixXd normalizedOutput = (normalizedInput.array().rowwise() * scale.array()).array().rowwise() + shift.array();

        std::cout << "scale ..." << std::endl;
        std::cout << scale << std::endl;

        std::cout << "shift ..." << std::endl;
        std::cout << shift << std::endl;

        std::cout << "normalizedOutput scaled ..." << std::endl;
        std::cout << normalizedOutput << std::endl;

        return normalizedOutput;
    }

    void setScale(const Eigen::VectorXd& newScale) {
        scale = newScale;
    }

    void setShift(const Eigen::VectorXd& newShift) {
        shift = newShift;
    }

    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        // Perform Linear Transformation.
        Eigen::MatrixXd output = normalize(input_data);

        print_string("Batch Normalize forward pass ...", true); 

        return output; // this becomes input to the next Node or next Layer.
    }

    // Leave the gradients as is for the scale and shift. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(const Eigen::MatrixXd& gradients) {
        int N = input_data.rows();

        // Compute the gradient with respect to the scale parameter (Gamma)
        dScale = (gradients.array() * normalizedInput.array()).colwise().sum();

        // Compute the gradient with respect to the shift parameter (Beta)
        dShift = gradients.colwise().sum();

        std::cout << "scale and shift\n";
        std::cout << scale << "\n\n";
        std::cout << shift << "\n";

        std::cout << "dScale and dShift gradients\n";
        std::cout << dScale << "\n\n";
        std::cout << dShift << "\n";

        // Compute the gradient with respect to the normalized input
        Eigen::MatrixXd dNormalizedInput = gradients.array().rowwise() * scale.array();

        std::cout << "dNormalizedInput\n";
        std::cout << dNormalizedInput << "\n";

        std::cout << "dNormalizedInput * minusMean\n";
        std::cout << dNormalizedInput.array() * minusMean.array()  << "\n";

        std::cout << "dNormalizedInput * minusMean rowwise sum\n";
        std::cout << (dNormalizedInput.array() * minusMean.array()).rowwise().sum() << "\n";;

        std::cout << "barchStdDev\n";
        std::cout << batchStdDev.array() << "\n";

        std::cout << "layerStdDev * layerStdDev\n";
        std::cout << (batchStdDev.array() * batchStdDev.array()) << "\n";

        // Compute the gradient with respect to the batch standard deviation
        Eigen::RowVectorXd dBatchStdDev = -(dNormalizedInput.array() * minusMean.array()).colwise().sum() / (batchStdDev.array() * batchStdDev.array());
 
        // Compute the gradient with respect to the batch variance
        Eigen::RowVectorXd dBatchVariance = 0.5 * (dBatchStdDev.array() / batchStdDev.array());

        std::cout << "dBatchVariance\n";
        std::cout << dBatchVariance << "\n";

        Eigen::MatrixXd dNormMinusMean1 = (dNormalizedInput.array() * (1.0 / (batchStdDev.array())).replicate(N,1)) + 
                                          (2.0 * minusMean.array() * (1.0/N * dBatchVariance.replicate(N, 1)).array());

        std::cout << "istd\n";
        std::cout << (1.0/(batchStdDev.array())).replicate(N,1)  << "\n";

        std::cout << "dsq\n";
        std::cout << 1.0/N * dBatchVariance.replicate(N, 1)  << "\n";

        std::cout << "xmu\n";
        std::cout << minusMean  << "\n";

        // Compute the gradient with respect to the batch mean
        Eigen::RowVectorXd dBatchMean = -1.0 * dNormMinusMean1.array().colwise().sum();

        std::cout << "dBatchMean\n";
        std::cout << dBatchMean << "\n";

        Eigen::MatrixXd dNormMinusMean2 = 1.0/N *  dBatchMean.replicate(N,1).array();

        std::cout << "dNormMinusMean2\n";
        std::cout << dNormMinusMean2 << "\n";

        // Compute the gradient with respect to the input
        Eigen::MatrixXd dInput = dNormMinusMean1.array() + dNormMinusMean2.array();
        std::cout << "dInput\n";
        std::cout << dInput << "\n";

        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        Eigen::MatrixXd pscale(1, this->M); 
        Eigen::MatrixXd gscale(1, this->M); 
        Eigen::MatrixXd pshift(1, this->M); 
        Eigen::MatrixXd gshift(1, this->M); 

        pscale.row(0) = scale;
        gscale.row(0) = dScale;

        pshift.row(0) = shift;
        gshift.row(0) = dShift;

        if (opt_scale == nullptr) {
            opt_scale = new Optimizer(optimizertype, learningRate);
        }

        if (opt_shift == nullptr) {
            opt_shift = new Optimizer(optimizertype, learningRate);
        }

        std::cout << "Updating Batch Normal scale  \n";
        opt_scale->adam(pscale, gscale, iter);
        std::cout << "Updating Batch Normal shift  \n";
        opt_shift->adam(pshift, gshift, iter);
        scale = pscale.row(0);
        shift = pshift.row(0);

        std::cout << "Updated scale:\n";
        std::cout << scale << "\n";
        std::cout << "Updated shift:\n";
        std::cout << shift << "\n";

        // initialize gradients for next iteration.
        dScale.setZero();
        dShift.setZero();

    }

    void forwardPass() { print_string("BatchNorm forward pass ...", true); }
    void backwardPass() {}
};


/*****************************************************************************************************
* Base Layer Normalization Function:
*   Suppose we have a sample input represented as a matrix of NxM where N=number of samples
*   and M=embedding vector size (features).  Our Batch normalization takes mean and variance
*   along the M dimension.
*****************************************************************************************************/
class LayerNorm : public BaseOperator {
private:
    Eigen::VectorXd scale; // (gamma)
    Eigen::VectorXd shift; // (beta)
    Eigen::VectorXd dScale; // gradient for the scale (gamma)
    Eigen::VectorXd dShift; // gradient for the shift (beta)

    // Cached data for backpropagation.
    Eigen::MatrixXd input_data; // NxW samples 
    Eigen::MatrixXd normalizedInput; // NxW samples 
    Eigen::MatrixXd minusMean;
    Eigen::VectorXd layerStdDev;
    int N = 0;

    Optimizer* opt_scale = nullptr; // for optimizer
    Optimizer* opt_shift = nullptr; // for optimizer

public:
    LayerNorm(int size) {
      // initialize gradients for next iteration.
        dScale.setZero();
        dShift.setZero();
    }

    // This assumes that the input is defined with NxM dimensionality.
    // Therefore the size of the parameters and thus gradients will be based on MxW where W is the number of weights to use.
    void setInitialWeights(int N) {

        // if size is already set, 
        // it means weights have previously already been set by an initial forward pass
        if (this->N != 0) return;

        this->N = N;

        // Initialize scaling and shifting parameters   
        scale.resize(this->N); // allocates memory
        shift.resize(this->N); // allocates memory

        // Initialize Weights & Biases
        heInitialization(scale);
        // heInitialization(shift);
        shift.setConstant(0.01);

        // Initialize Gradients     
        dScale.setZero();
        dShift.setZero();
    }

    Eigen::MatrixXd normalize(const Eigen::MatrixXd& input_data) {

        setInitialWeights(input_data.rows());

        std::cout << input_data << std::endl;

        // Calculate layer mean along the M dimension, but along the N dimension.
        Eigen::VectorXd layerMean = input_data.rowwise().mean();

        std::cout << "Mean ..." << std::endl;
        std::cout << layerMean << std::endl;

        // Calculate: X - mean
        minusMean = input_data.colwise() - layerMean;

        std::cout << "minusMean\n";
        std::cout << minusMean << "\n";

        // Calculate batch variance along the M dimension
        Eigen::VectorXd layerVariance = minusMean.array().square().rowwise().mean();

        std::cout << "Variance ..." << std::endl;
        std::cout << layerVariance << std::endl;

        // Add a small epsilon for numerical stability
        double epsilon = 1e-8;
        Eigen::VectorXd epsilonVector = Eigen::VectorXd::Constant(layerVariance.size(), epsilon);

        std::cout << "Epsilon ..." << std::endl;
        std::cout << epsilonVector << std::endl;

        // Calculate batch standard deviation across the M dimension
        layerStdDev = (layerVariance + epsilonVector).array().sqrt();

        std::cout << "stdDev ..." << std::endl;
        std::cout << layerStdDev << std::endl;

        // Normalize the inputs along the M dimension
        normalizedInput = minusMean.array().colwise()  / layerStdDev.array();

        std::cout << "normalizedInput along the N  ..." << std::endl;
        std::cout << normalizedInput << std::endl;

        // Scale and shift the normalized inputs
        Eigen::MatrixXd normalizedOutput = (normalizedInput.array().colwise() * scale.array()).array().colwise() + shift.array();

        std::cout << "scale ..." << std::endl;
        std::cout << scale << std::endl;

        std::cout << "shift ..." << std::endl;
        std::cout << shift << std::endl;

        std::cout << "normalizedOutput scaled ..." << std::endl;
        std::cout << normalizedOutput << std::endl;

        return normalizedOutput;
    }
    

    void setScale(const Eigen::VectorXd& newScale) {
        scale = newScale;
    }

    void setShift(const Eigen::VectorXd& newShift) {
        shift = newShift;
    }

    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        // Perform Linear Transformation.
        Eigen::MatrixXd output = normalize(input_data);

        print_string("Layer Normalize forward pass ...", true); 

        return output; // this becomes input to the next Node or next Layer.
    }

    // Leave the gradients as is for the scale and shift. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(const Eigen::MatrixXd& gradients) {
        int W = input_data.cols();

        std::cout << "normalizedInput \n";
        std::cout << normalizedInput << "\n";
        // Compute the gradient with respect to the scale parameter (Gamma)
        dScale = (gradients.array() * normalizedInput.array()).rowwise().sum();

        // Compute the gradient with respect to the shift parameter (Beta)
        dShift = gradients.rowwise().sum();

        std::cout << "scale and shift gradients\n";
        std::cout << scale << "\n\n";
        std::cout << shift << "\n";

        std::cout << "dScale and dShift gradients\n";
        std::cout << dScale << "\n\n";
        std::cout << dShift << "\n";

        // Compute the gradient with respect to the normalized input
        Eigen::MatrixXd dNormalizedInput = gradients.array().colwise() * scale.array();

        std::cout << "dNormalizedInput\n";
        std::cout << dNormalizedInput << "\n";

        std::cout << "dNormalizedInput * minusMean\n";
        std::cout << dNormalizedInput.array() * minusMean.array()  << "\n";

        std::cout << "dNormalizedInput * minusMean rowwise sum\n";
        std::cout << (dNormalizedInput.array() * minusMean.array()).rowwise().sum() << "\n";;

        std::cout << "layerStdDev\n";
        std::cout << layerStdDev.array() << "\n";

        std::cout << "layerStdDev * layerStdDev\n";
        std::cout << (layerStdDev.array() * layerStdDev.array()) << "\n";

        // Compute the gradient with respect to the layer standard deviation
        Eigen::VectorXd dLayerStdDev = -(dNormalizedInput.array() * minusMean.array()).rowwise().sum() / (layerStdDev.array() * layerStdDev.array());

        std::cout << "dLayerStdDev\n";
        std::cout << dLayerStdDev << "\n";

        // Compute the gradient with respect to the layer variance
        Eigen::VectorXd dLayerVariance = 0.5 * (dLayerStdDev.array() / layerStdDev.array());

        std::cout << "dLayerVariance\n";
        std::cout << dLayerVariance << "\n";

        Eigen::MatrixXd dNormMinusMean1 = (dNormalizedInput.array() * (1.0 / (layerStdDev.array())).replicate(1,W)) + 
                                          (2.0 * minusMean.array() * (1.0/W * dLayerVariance.replicate(1,W)).array());

        std::cout << "dxmu1\n";
        std::cout << (dNormalizedInput.array() * (1.0 / (layerStdDev.array())).replicate(1,W)) << "\n";
        std::cout << "dxmu2\n";
        std::cout <<  (2.0 * minusMean.array() * (1.0/W * dLayerVariance.replicate(1,W)).array()) << "\n";

        std::cout << "dNormMinusMean1\n";
        std::cout << dNormMinusMean1 << "\n";

        std::cout << "istd\n";
        std::cout << (1.0/(layerStdDev.array() * layerStdDev.array())).replicate(1,W)  << "\n";

        std::cout << "dsq\n";
        std::cout << 1.0/N * dLayerVariance.replicate(1, 2)  << "\n";

        std::cout << "xmu\n";
        std::cout << minusMean  << "\n";

        // Compute the gradient with respect to the batch mean
        Eigen::VectorXd dLayerMean = -1.0 * dNormMinusMean1.array().rowwise().sum();

        std::cout << "dLayerMean\n";
        std::cout << dLayerMean << "\n";

        Eigen::MatrixXd dNormMinusMean2 = 1.0/W *  dLayerMean.replicate(1,W).array();

        std::cout << "dNormMinusMean2\n";
        std::cout << dNormMinusMean2 << "\n";

        // Compute the gradient with respect to the input
        Eigen::MatrixXd dInput = dNormMinusMean1.array() + dNormMinusMean2.array();

        std::cout << "dInput\n";
        std::cout << dInput << "\n";

        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        Eigen::MatrixXd pscale(this->N, 1); 
        Eigen::MatrixXd gscale(this->N, 1); 
        Eigen::MatrixXd pshift(this->N, 1); 
        Eigen::MatrixXd gshift(this->N, 1); 

        pscale.col(0) = scale;
        gscale.col(0) = dScale;

        pshift.col(0) = shift;
        gshift.col(0) = dShift;

        if (opt_scale == nullptr) {
            opt_scale = new Optimizer(optimizertype, learningRate);
        }

        if (opt_shift == nullptr) {
            opt_shift = new Optimizer(optimizertype, learningRate);
        }

        std::cout << "Updating Layer Normal scale  \n";
        opt_scale->adam(pscale, gscale, iter);
        std::cout << "Updating Layer Normal shift  \n";
        opt_shift->adam(pshift, gshift, iter);
        scale = pscale.col(0);
        shift = pshift.col(0);

        std::cout << "Updated scale:\n";
        std::cout << scale << "\n";
        std::cout << "Updated shift:\n";
        std::cout << shift << "\n";

        // initialize gradients for next iteration.
        dScale.setZero();
        dShift.setZero();

    }

    void forwardPass() { print_string("LayerNorm forward pass ...", true); }
    void backwardPass() {}
};

/*****************************************************************************************************
* Base Activation Functions
*****************************************************************************************************/
class Activation : public BaseOperator {
private:

    // Cached data for backpropagation.
    Eigen::MatrixXd input_data; // NxW samples 

    std::string activationtype = "leakyrelu";
    float alpha = 0.01; // for leakyReLU
public:

    Activation(const std::string& activationtype = "leakyrelu") {
        this->activationtype = activationtype;
    }

    Activation(const std::string& activationtype = "leakyrelu", const float alpha=0.01) {
        this->activationtype = activationtype;
        this->alpha = alpha;
    }

    // Here, instead of using the term logits, let's just use x.
    Eigen::MatrixXd sigmoid(const Eigen::MatrixXd& x) {
        return (1.0 / (1.0 + (-x).array().exp())).matrix();
    }

    /*****************************************************************************************************
     * So, the gradient of the sigmoid function with respect to its input (z) can be expressed as follows:
     * dy/dz = propagated_gradient * sigmoid(z) * (1 - sigmoid(z))
     * Or if output (y) is cached, where y = sigmoid(z)
     * then we use the output such that:
     * dy/dz = propagated_gradient * y * (1 - y)
     *****************************************************************************************************/
    Eigen::MatrixXd sigmoidGradient(const Eigen::MatrixXd& gradients, const Eigen::MatrixXd& output_data) {
        Eigen::MatrixXd dInput = gradients.array() * output_data.array() * ( 1 - output_data.array());
        return dInput;
    }

    Eigen::MatrixXd tanh(const Eigen::MatrixXd& x) {
        return x.array().tanh();
    }

    /*****************************************************************************************************
     * So, the gradient of the tanh function with respect to its input (z) can be expressed as follows:
     * dy/dz = propagated_gradient * (1 - tanh(z)^2)
     * Or if output (y) is cached, where y = tanh(z)
     * then we use the output such that:
     * dy/dz = propagated_gradient * y * (1 - y^2)
     *****************************************************************************************************/
    Eigen::MatrixXd tanhGradient(const Eigen::MatrixXd& gradients, const Eigen::MatrixXd& output_data) {
        Eigen::MatrixXd dInput =  gradients.array() *  ( 1 - output_data.array().pow(2));
        return dInput;
    }

    Eigen::MatrixXd relu(const Eigen::MatrixXd& x) {
        return x.array().max(0.0);
    }

    /*****************************************************************************************************
     * So, the gradient of the ReLU function with respect to its input (z) can be expressed as follows:
     * dy/dz = propagated_gradient * 1 for z > 0
     * dy/dz = propagated_gradient * 0 for z <= 0
     *****************************************************************************************************/
    Eigen::MatrixXd reluGradient(const Eigen::MatrixXd& gradients) {
        Eigen::MatrixXd dInput = this->input_data.array().max(0.0).cast<bool>().cast<double>() * gradients.array();
        return dInput;
    }

    Eigen::MatrixXd leakyReLU(const Eigen::MatrixXd& x, float alpha) {
        return x.array().max(alpha * x.array());
    }

    /*****************************************************************************************************
     * So, the gradient of the LeakyReLU function with respect to its input (z) can be expressed as follows:
     * dy/dz = propagated_gradient * 1 for z > 0
     * dy/dz = propagated_gradient * alpha for z <= 0
    *****************************************************************************************************/
    Eigen::MatrixXd leakyReluGradient(const Eigen::MatrixXd& gradients) {
        std::cout << "leaky Relu Gradient ...\n";
        std::cout << "input:\n";
        std::cout << input_data << "\n";
        std::cout << "gradient:\n";
        std::cout << gradients << "\n";
        Eigen::MatrixXd dInput = this->input_data.array().max(0.0).cast<bool>().cast<double>().max(alpha) * gradients.array();
        std::cout << input_data << "\n\n";
        std::cout << "dInput\n\n";
        std::cout << dInput << "\n";
        return dInput;
    }

    Eigen::MatrixXd gelu(const Eigen::MatrixXd& x) {
        return 0.5 * x.array() * (1.0 + ((x.array() * std::sqrt(2.0 / M_PI)).tanh()));
    }

    /*****************************************************************************************************
     * Gelu Gradient ...
    *****************************************************************************************************/
    Eigen::MatrixXd geluGradient(const Eigen::MatrixXd& gradients) {
        // Calculate the coefficient used in the GELU formula
        double coefficient = sqrt(2.0 / M_PI);
        // Compute the cumulative distribution function (CDF) part of the GELU gradient
        Eigen::MatrixXd cdf = 0.5 * (1.0 + (gradients.array() / coefficient).tanh());
        // Compute the probability density function (PDF) part of the GELU gradient
        Eigen::MatrixXd pdf = exp(-0.5 * gradients.array().square()) / coefficient;
        // Combine the CDF and PDF components to obtain the final gradient values
        // Apply element-wise operations on arrays: add CDF, multiply x by PDF, add a term based on x^3
        return 0.5 * (1.0 + (cdf.array() + gradients.array() * pdf.array() + 0.044715 * gradients.array().cube())).matrix();
    }

    Eigen::MatrixXd computeActivation(const Eigen::MatrixXd& input_data) { 
        Eigen::MatrixXd output;
        if (activationtype == "sigmoid") {
            output = sigmoid(input_data);
        } else
        if (activationtype == "tanh") {
            output = tanh(input_data);
        } else
        if (activationtype == "relu") {
            std::cout << "Relu\n";
            output = relu(input_data);
        } else
        if (activationtype == "leakyrelu") {
            std::cout << "leakyRelu\n";
            output = leakyReLU(input_data, alpha);
        } else
        if (activationtype == "gelu") {
            output = gelu(input_data);
        } else
        if (activationtype == "softmax") {
            std::cout << "softmax\n";
            output = softmax(input_data);
        }
        std::cout << "Activation output\n";
        std::cout << output << "\n";
        return output; // this becomes input to the next Node or next Layer.
    }

    Eigen::MatrixXd computeGradient(const Eigen::MatrixXd& gradients, const Eigen::MatrixXd& output_data) {
        Eigen::MatrixXd dInput;
        if (activationtype == "sigmoid") {
            dInput = sigmoidGradient(gradients, output_data);
        } else
        if (activationtype == "tanh") {
            dInput = tanhGradient(gradients, output_data);
        } else
        if (activationtype == "relu") {
            std::cout << "relu\n";
            dInput = reluGradient(gradients);
        } else
        if (activationtype == "leakyrelu") {
            std::cout << "leakyrelu\n";
            dInput = leakyReluGradient(gradients);
        } else
        if (activationtype == "gelu") {
            dInput = geluGradient(gradients);
        } else
        if (activationtype == "softmax") {
            std::cout << "softmax\n";
            dInput = softmaxGradient(gradients, output_data);
        } 
        return dInput; // this becomes input to the next Node or next Layer backward.
    }

    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        // Perform Activation
        Eigen::MatrixXd output = computeActivation(input_data);

        print_string("Activation forward pass ...", true); 

        return output; // this becomes input to the next Node or next Layer.
    }

    Eigen::MatrixXd backward(const Eigen::MatrixXd& gradient, const Eigen::MatrixXd& output_data) {

        // Perform Gradient
        Eigen::MatrixXd dInput = computeGradient(gradient, output_data);

        print_string("Activation backward pass ...", true); 

        return dInput; // this becomes input to the next Node or next Layer.
    }

    void forwardPass() { print_string("Activate forward pass ...", true); }
    void backwardPass() {}
};

/*****************************************************************************************************
* Base Loss Functions
*****************************************************************************************************/
class Loss : public BaseOperator {
private:
    std::string losstype = "mse";
public:

    Loss(const std::string& losstype = "mse") {
        this->losstype = losstype;
    }

    // Mean Squared Error. Returns 1x1 matrix (scalar)
    Eigen::MatrixXd mse(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Eigen::VectorXd diff = predicted - target;
        double mse = diff.array().square().rowwise().sum().mean();
        return Eigen::MatrixXd::Constant(1, 1, mse);
    }

    Eigen::MatrixXd mseGradient(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        std::cout << "Predicted ...\n";
        std::cout << predicted << "\n";
        std::cout << "target ...\n";
        std::cout << target << "\n";
        return 2 * (predicted - target);
    }

    // Binary Cross Entropy.  Returns 1x1 matrix (scalar)
    Eigen::MatrixXd bce(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Eigen::MatrixXd loss = -target.array() * predicted.array().log() - (1.0 - target.array()) * (1.0 - predicted.array()).log();
        double averageLoss = loss.mean();
        return Eigen::MatrixXd::Constant(1, 1, averageLoss);
    }

    Eigen::MatrixXd bceGradient(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Eigen::MatrixXd gradient = (predicted - target).array() / (predicted.array() * (1 - predicted.array()));
        return gradient;
    }


    // For Loss Categorial Cross Entropy. Usually, we use Softmax.
    // If predicted and target has NxC dimension where C is number of classes, then result will be Nx1.
    Eigen::MatrixXd cce(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        int N = predicted.rows(); // Number of samples
        int C = predicted.cols(); // Number of classes
        Eigen::MatrixXd loss(N, 1);
        for (int i = 0; i < N; ++i) {
            double sampleLoss = 0.0;
            for (int j = 0; j < C; ++j) {
                sampleLoss -= target(i, j) * std::log(predicted(i, j));
            }
            loss(i, 0) = sampleLoss;
        }
        return loss;
    }

    Eigen::MatrixXd cceGradient(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        int numClasses = predicted.rows();
        Eigen::MatrixXd gradient(numClasses, 1);
        for (int i = 0; i < numClasses; ++i) {
            gradient(i, 0) = predicted(i, 0) - target(i, 0);
        }
        return gradient;
    }

    /*
    Eigen::MatrixXd hingeLoss(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        return predicted.array().max(0.0) - predicted.array() * target.array() + (1 - target.array()).max(0.0);
    } */

    // For Support Vectors (not necessarily for Neural)
    Eigen::MatrixXd hingeLoss(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Eigen::MatrixXd loss = (1.0 - predicted.array() * target.array()).cwiseMax(0.0);
        return loss;
    }

    Eigen::MatrixXd hingeLossGradient(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        return (predicted.array() * target.array() < 1).select(-target, 0);
    }

    Eigen::MatrixXd computeLoss(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) { 
        Eigen::MatrixXd output;
        if (losstype == "mse") {
            output = mse(predicted, target);
        } else
        if (losstype == "bce") {
            output = bce(predicted, target);
        } else
        if (losstype == "cce") {
            output = cce(predicted, target);
        } else
        if (losstype == "hingeLoss") {
            output = hingeLoss(predicted, target);
        } 
        return output; // this becomes input to the next Node or next Layer forward.
    }

    Eigen::MatrixXd computeGradients(const Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) { 
        Eigen::MatrixXd gradients;
        if (losstype == "mse") {
            gradients = mseGradient(predicted, target);
        } else
        if (losstype == "bce") {
            gradients = bceGradient(predicted, target);
        } else
        if (losstype == "cce") {
            gradients = cceGradient(predicted, target);
        } else
        if (losstype == "hingeLoss") {
            gradients = hingeLossGradient(predicted, target);
        } 
        return gradients; // this becomes input to the next Node or next Layer backward.
    }

    void forwardPass() { print_string("Loss forward pass ...", true); }
    void backwardPass() {}

};

class Dropout : public BaseOperator {
public:
    Dropout(int size) {
        // Initialize scaling and shifting parameters
    }

    void forwardPass() { print_string("Scale forward pass ...", true); }
    void backwardPass() {}
};

/*****************************************************************************************************
* Base Attention Head Layer
*****************************************************************************************************/
class Attention : public BaseOperator {
private:

    Eigen::MatrixXd input_data; // NxW samples, by using & 
    Linear* Q  = nullptr;
    Linear* K  = nullptr;
    Linear* V  = nullptr;
    Linear* Wo = nullptr; // this extra weight matrix will align the output dimension the same as the input.

    Eigen::MatrixXd Qout;
    Eigen::MatrixXd Kout;
    Eigen::MatrixXd Vout;

    Eigen::MatrixXd QKsoft;
    Eigen::MatrixXd QKsoftV;

    int N = 0;  // number of samples
    int M = 0;  // number of features (embedding vector size)
    int W = 0;  // number of weights (or number of features)
    int H = 1;  // number of heads
    int Dk = 0; // number of dimensions per head (M/H)

    bool bias = false;

public:
    Attention(int heads = 1, int size = 3, bool bias = false)  {
        this->W = size;
        this->H = heads;
        this->bias = bias;
        print_string("Attention operation ...", true);
    }

    // While the parameter weight has dimension MxW,  the resulting transformation has dimension of NxW.
    // We only need the M dimension from an NxM input to generate parameter matrix.
    // where weights is NxW and bias is W.
    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        this->N = input_data.rows();
        this->M = input_data.cols();
        this->Dk = this->M / this->H;

        std::cout << "Size of input:" << this->input_data.size() << "\n";

        if (Q == nullptr || K == nullptr || V == nullptr || Wo == nullptr) {
            Q  = new Linear(this->W, bias);
            K  = new Linear(this->W, bias);
            V  = new Linear(this->W, bias);
            Wo = new Linear(this->M, bias);
        }

        // Perform Linear Transformation.
        Qout = Q->forward(input_data);
        Kout = K->forward(input_data);
        Vout = V->forward(input_data);

        std::cout << "Just Qout\n";
        std::cout << Qout << "\n";

        std::cout << "Just Kout\n";
        std::cout << Kout << "\n";

        std::cout << "Just Vout\n";
        std::cout << Vout << "\n";

        // MatMul (QK^T)
        Eigen::MatrixXd QK = BaseOperator::matmul(Qout, Kout.transpose());

        std::cout << "Just QK (matmul)\n";
        std::cout << QK << "\n";

        // Include some Masking (still to be implemented)

        // Scale sqrt(Dk)
        QK = QK.array() / sqrt(Dk);

        std::cout << "Just QK / sqrt(Dk\n";
        std::cout << QK << "\n";

        // Mask if required (Decoder)
        // ...

        // Perform Softmax
        QKsoft = BaseOperator::softmax(QK);

        std::cout << "Softmax QKsoft output\n";
        std::cout << QKsoft << "\n";

        std::cout << "Vout output\n";
        std::cout << Vout << "\n";

        // Include dropout (still to be implemented)

        // Perform matmul with V
        QKsoftV = BaseOperator::matmul(QKsoft, Vout);

        // Perform another transform to align dimension.
        Eigen::MatrixXd output = Wo->forward(QKsoftV);

        print_string("Attention forward pass output ...", true); 
        std::cout << output << "\n\n";

        return output; // this becomes input to the next Node or next Layer.
    }


    // Leave the gradients as is. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(Eigen::MatrixXd& gradients) { 

        // Gradient with Respect to W linear operations.
        Eigen::MatrixXd WodInput = Wo->backward(gradients); 

        // Gradient with Respect to QKsoft (matmul operation)
        Eigen::MatrixXd dQKsoft = BaseOperator::matmul(WodInput, Vout.transpose());

        std::cout << "dQKsoft gradient\n";
        std::cout << dQKsoft << "\n";

        // Gradient with Respect to Vout (matmul operation)
        Eigen::MatrixXd VmInput = BaseOperator::matmul(QKsoft.transpose(), WodInput);

        std::cout << "VmInput gradient\n";
        std::cout << VmInput << "\n";



        // Propagate Gradient to softmax operation.
        Eigen::MatrixXd dInput = BaseOperator::softmaxGradient(dQKsoft, QKsoft);

        std::cout << "dInput gradient\n";
        std::cout << dInput << "\n";

        // Propagate Gradient to scale operation.
        Eigen::MatrixXd dScale = -0.5 * dInput.array() * (1.0 / std::sqrt(Dk));

        std::cout << "dScale gradient\n";
        std::cout << dInput << "\n";

        // Gradient with Respect to Q (matmul operation)
        Eigen::MatrixXd QdQK = BaseOperator::matmul(dScale, Kout); // Kout was already tranposed during forward.

        std::cout << "QdQK gradient\n";
        std::cout << QdQK << "\n";

        // Gradient with Respect to V (matmul operation)
        Eigen::MatrixXd KdQK = BaseOperator::matmul(Qout.transpose(), dScale);

        std::cout << "KdQK gradient\n";
        std::cout << KdQK << "\n";


        // Propagate Gradient to the Q,K,V linear operations.
        // std::cout << " Backprop to Q ...\n";
        Eigen::MatrixXd  QdInput = Q->backward(QdQK);
        // std::cout << " Backprop to K ...\n";
        Eigen::MatrixXd  KdInput = V->backward(KdQK);
        // std::cout << " Backprop to V ...\n";
        Eigen::MatrixXd  VdInput = V->backward(VmInput); 

        std::cout << " Done Backprop to Q, K, V ...\n";

        std::cout << "VdInput gradient\n";
        std::cout << VdInput << "\n";

        std::cout << "QdInput gradient\n";
        std::cout << QdInput << "\n";

        std::cout << "KdInput gradient\n";
        std::cout << KdInput << "\n";

        dInput = QdInput + KdInput + VdInput;

        std::cout << "dInput gradient\n";
        std::cout << dInput << "\n\n";

        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        Q->updateParameters(optimizertype, learningRate, iter);
        K->updateParameters(optimizertype, learningRate, iter);
        V->updateParameters(optimizertype, learningRate, iter);
        Wo->updateParameters(optimizertype, learningRate, iter);
    }

    void forwardPass() { print_string("Attention forward pass ...", true); }
    void backwardPass() {}
    
};

/*****************************************************************************************************
* Base Multi-Head Attention Layer
*****************************************************************************************************/
class MultiAttention : public BaseOperator {
private:
    Eigen::MatrixXd input_data; // NxW samples, by using & 
    std::vector<Attention*> M1;

    bool bias = true;

    int N = 0;  // number of samples
    int M = 0;  // number of features (embedding vector size)
    int W = 0;  // number of weights (or number of features)
    int H = 1;  // number of heads
    int Dk = 0; // number of dimensions per head (M/H)
    int split = 0; // number of values in an array to jump.

    std::string activationtype = "leakyrelu";
    float alpha = 0.01; // for leakyReLU

public:
    MultiAttention(int heads = 3, int size = 3, bool bias = false)  {
        this->W = size;
        this->H = heads;
        this->bias = bias;
        // M1.setZero();
        print_string("MultiAttention operation ...", true);
    }

    // While the parameter weight has dimension MxW,  the resulting transformation has dimension of NxW.
    // We only need the M dimension from an NxM input to generate parameter matrix.
    // where weights is NxW and bias is W.
    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        this->N = input_data.rows();
        this->M = input_data.cols();

        std::cout << "Size of input:" << this->input_data.size() << "\n";
        std::cout << "Size of Head:" << H << "\n";

        if (M1.empty()) {
            for (int i = 0; i < this->H; i++) {
                Attention* A1  = new Attention(1, this->W);
                M1.push_back(A1);
            }
        }


        this->Dk = this->M / this->H;
        int splits = 0; 

        std::cout << "Size of DK ..." << Dk << "\n";

        std::vector<Eigen::MatrixXd> outputs;

        for (int i = 0; i < this->H; i++) {
            splits = this->Dk * this->N * i; 
            std::cout << "Loop " << i << " ... split " << splits << "\n";
            Eigen::Map<Eigen::MatrixXd> O(this->input_data.data()+splits, N, Dk);
            outputs.push_back(O);
            
        }  

        std::cout << "Outputs" << "\n";
        for (int i = 0; i < this->H; i++) {
            std::cout << "FIrst head: " << i << "\n";
            std::cout << outputs[i] << "\n";
        }

        std::cout << "Now performing Forward pass \n";

        // Perform Forward pass.
        for (int i = 0; i < this->H; i++) {
            /// std::cout << "Attention forward entering ..." << i << "\n";
            outputs[i] = M1[i]->forward(outputs[i]);
            // std::cout << "Attention forward done ..." << i << "\n";
        }

        std::cout << "Now performing concatenation \n";

        int totalCols = 0;
        for (const auto& output : outputs) {
            totalCols += output.cols();
        }

        // now concatenate
        Eigen::MatrixXd concatenated_output = Eigen::MatrixXd::Zero(outputs[0].rows(), totalCols);
        int colOffset = 0;
        for (const auto& output : outputs) {
            concatenated_output.block(0, colOffset, output.rows(), output.cols()) = output;
            colOffset += output.cols();
        }

        std::cout << "MultiAttention forward pass done. \n";

        return concatenated_output;
    }

    // Leave the gradients as is. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(Eigen::MatrixXd& gradients) { 
        // Propagate Gradient to multihead operations.
        Eigen::MatrixXd  dInput = Eigen::MatrixXd::Zero(gradients.rows(), gradients.cols());

        std::cout << "Inside MultiAttention backward function ...\n";
        // Perform MultiAttention backward pass.
        for (int i = 0; i < this->H; i++) {
            std::cout << "calling attention backprop ...\n";
            std::cout << "attention gradient output \n";
            dInput += M1[i]->backward(gradients);
        }
        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        for (int i = 1; i < this->H; i++) {
            M1[i]->updateParameters(optimizertype, learningRate, iter);
        }
    }

    void forwardPass() { print_string("MultiAttention forward pass ...", true); }
    void backwardPass() {}

};

/*****************************************************************************************************
* Base FeedForward  Layer
*****************************************************************************************************/
class FeedForward : public BaseOperator {
private:
    Eigen::MatrixXd input_data; // NxW samples, by using & 
    Linear* L1 = nullptr;
    Linear* L2 = nullptr; // required to align the dimension similar to the input
    Activation* A1 = nullptr;

    Eigen::MatrixXd L1out; // Cache output for use by activation backprop

    bool bias = true;
    int W = 0;
    int N = 0;
    int M = 0;

    std::string activationtype = "leakyrelu";
    float alpha = 0.01; // for leakyReLU

public:

    FeedForward(int size = 3, bool bias = true, const std::string& activationtype = "leakyrelu") {
        this->activationtype = activationtype;
        this->W = size;
        this->bias = bias;
        print_string("FeedForward operation ...", true);
    }

    FeedForward(int size = 3, bool bias = true, const std::string& activationtype = "leakyrelu", const float alpha=0.01) {
        this->activationtype = activationtype;
        this->alpha = alpha;
        this->W = size;
        this->bias = bias;
        print_string("FeedForward operation ...", true);
    }

    // While the parameter weight has dimension MxW,  the resulting transformation has dimension of NxW.
    // We only need the M dimension from an NxM input to generate parameter matrix.
    // where weights is NxW and bias is W.
    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        this->N = input_data.rows();
        this->M = input_data.cols();

        std::cout << "Size of input:" << this->input_data.size() << "\n";

        if (L1 == nullptr || L2 == nullptr || A1 == nullptr) {
            L1 = new Linear(this->W, bias);
            L2 = new Linear(this->M, bias); // requires to have dimension as the feedforward input
            A1 = new Activation(this->activationtype, this->alpha);
        }

        // Perform Linear Transformation.
        L1out = L1->forward(input_data);  // Cache output for use by backward activation later
        Eigen::MatrixXd A1out = A1->forward(L1out);
        Eigen::MatrixXd output = L2->forward(A1out);

        return output;
    }

    // Leave the gradients as is. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(Eigen::MatrixXd& gradients) { 
        // Propagate Gradient to feedforward operations.
        Eigen::MatrixXd  L2gradients = L2->backward(gradients); 
        Eigen::MatrixXd  A1gradients = A1->backward(L2gradients, L1out);
        Eigen::MatrixXd  dInput = L1->backward(A1gradients);
        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        L1->updateParameters(optimizertype, learningRate, iter);
        L2->updateParameters(optimizertype, learningRate, iter);
    }

    void forwardPass() { print_string("FeedForward forward pass ...", true); }
    void backwardPass() {}

};

/*****************************************************************************************************
* Base Encoder  Layer
*****************************************************************************************************/
class Encoder : public BaseOperator {
private:
    Eigen::MatrixXd input_data; // NxW samples, by using & 
    MultiAttention* M1 = nullptr;
    LayerNorm* LN1 = nullptr;
    FeedForward* F1 = nullptr;
    LayerNorm* LN2 = nullptr;

    Eigen::MatrixXd M1out; // Cache output for use by attention backprop
    Eigen::MatrixXd F1out; // Cache output for use by feedforward backprop
    Eigen::MatrixXd LN1out; // Cache output for use by feedforward backprop

    bool bias = true;
    int H = 0;
    int W = 0;
    int N = 0;

    std::string activationtype = "leakyrelu";
    float alpha = 0.01; // for leakyReLU

public:

    Encoder(int heads = 1, int size = 3, bool bias = true, const std::string& activationtype = "leakyrelu") {
        this->activationtype = activationtype;
        this->W = size;
        this->bias = bias;
        this->H = heads;
        print_string("Encoder operation ...", true);
    }

    Encoder(int heads = 1, int size = 3, bool bias = true, const std::string& activationtype = "leakyrelu", const float alpha=0.01) {
        this->activationtype = activationtype;
        this->alpha = alpha;
        this->W = size;
        this->bias = bias;
        this->H = heads;
        print_string("Encoder operation ...", true);
    }

    // While the parameter weight has dimension MxW,  the resulting transformation has dimension of NxW.
    // We only need the M dimension from an NxM input to generate parameter matrix.
    // where weights is NxW and bias is W.
    Eigen::MatrixXd forward(Eigen::MatrixXd& input_data) { 

        // Cache for later back propagation.
        this->input_data = input_data;

        this->N = input_data.rows();

        std::cout << "Size of input:" << this->input_data.size() << "\n";

        if (M1 == nullptr || LN1 == nullptr || F1 == nullptr || LN2 == nullptr) {
            LN2 = new LayerNorm(this->W);
            F1  = new FeedForward(this->W, this->bias, this->activationtype,  this->alpha);
            LN1 = new LayerNorm(this->W); 
            M1  = new MultiAttention(this->H, this->W);
        }

        // Perform Linear Transformation.
        M1out = M1->forward(input_data);

        std::cout << "Input Data ....\n";
        std::cout << input_data << "\n";

        std::cout << "\nEncoder Attention forward output ...\n";
        std::cout << M1out << "\n";

        Eigen::MatrixXd InputM1out = M1out.array() + input_data.array();

        std::cout << "\nEncoder Add 1 forward output ...\n";
        std::cout << InputM1out << "\n";

        LN1out = LN1->forward(InputM1out);

        std::cout << "\nEncoder LN1 forward output ...\n";
        std::cout << LN1out << "\n";

        F1out = F1->forward(LN1out);

        std::cout << "\nEncoder FeedForward forward output ...\n";
        std::cout << F1out << "\n";

        Eigen::MatrixXd LN1F1out = F1out.array() + LN1out.array();

        std::cout << "\nEncoder Add 2 forward output ...\n";
        std::cout << LN1F1out << "\n";

        Eigen::MatrixXd output = LN2->forward(LN1F1out);

        std::cout << "\nEncoder LN2 forward output ...\n";
        std::cout << output << "\n\n";

        return output;
    }

    // Leave the gradients as is. They are cached in the Node. 
    // They will be used to update the parameters in next parallel operations.
    // the dInput is the gradient we propagate to source Nodes in the graph;
    // while the parameter gradients get cached to be used to update the parameters later.
    Eigen::MatrixXd backward(Eigen::MatrixXd& gradients) { 

        std::cout << "\nEntering Encoder backpropagation ...\n";
        // Propagate Gradient to Encoder.
        Eigen::MatrixXd  LN2gradients = LN2->backward(gradients); 

        std::cout << "\nEncoder LN2 backprop output ...\n";
        std::cout << LN2gradients << "\n\n";
    
        Eigen::MatrixXd F1LN2gradients = LN2gradients.array(); // F1out.array() * LN2gradients.array();

        Eigen::MatrixXd F1gradients = F1->backward(F1LN2gradients);

        std::cout << "\nEncoder F1 backprop output ...\n";
        std::cout << F1gradients << "\n\n";

        Eigen::MatrixXd InputLN2gradients = LN2gradients.array(); // LN1out.array() * LN2gradients.array();

        std::cout << "\nEncoder input * F1 backprop output ...\n";
        std::cout << InputLN2gradients << "\n\n";

        F1gradients = InputLN2gradients + F1gradients;

        std::cout << "\nEncoder (InputLN1gradients + F1gradients) backprop output ...\n";
        std::cout << F1gradients << "\n\n";

        Eigen::MatrixXd  LN1gradients = LN1->backward(F1gradients);

        std::cout << "\nEncoder LN1 backprop output ...\n";
        std::cout << LN1gradients << "\n\n";

        Eigen::MatrixXd M1LN1gradients = LN1gradients.array(); // A1out.array() * LN1gradients.array();

        Eigen::MatrixXd  M1gradients =  M1->backward(M1LN1gradients);

        std::cout << "\nEncoder A1 backprop output ...\n";
        std::cout << M1gradients << "\n\n";

        Eigen::MatrixXd LN1outLN1gradients = LN1gradients.array(); // input_data.array() * LN1gradients.array();

        Eigen::MatrixXd dInput = LN1outLN1gradients + M1gradients;      

        std::cout << "\nEncoder dInput ...\n";
        std::cout << dInput << "\n";

        return dInput;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        LN1->updateParameters(optimizertype, learningRate, iter);
        LN2->updateParameters(optimizertype, learningRate, iter);
        F1->updateParameters(optimizertype, learningRate, iter);
        M1->updateParameters(optimizertype, learningRate, iter);
    }

    void forwardPass() { print_string("Encoder forward pass ...", true); }
    void backwardPass() {}

};

/********************************************************************************************
* NodeFactory
********************************************************************************************/

enum class NodeType {
    Input,
    Hidden,
    Output
};

class Node {
private:
    int id;
   // Graph* graph; // Pointer to the graph that the node belongs to
    std::unordered_set<Node*> outputs;
    std::unordered_set<Node*> inputs;
    std::vector<std::shared_ptr<BaseOperator>> operations;
    Eigen::MatrixXd input_data;
    Eigen::MatrixXd output_data;
    Eigen::MatrixXd dInput;
    // Reduction* reduce_op = nullptr;
    ssize_t repeat = 1;
    std::string reduce = "add";


public:
    std::string name;
    NodeType type;

    Node(const std::string& name, NodeType type, const py::array_t<double>& embedding = {})
        : name(name), type(type) {
        if (embedding.size() != 0) {
            setData(embedding);
        } else {
            dInput.setZero();   
            input_data.setZero();  
            output_data.setZero();
        }
    }

    std::string getName() {
        return this->name;
    }

    NodeType nodeType() {
        return this->type;
    }

    // The input is assumed to have NxM where N=number of samples, M=embedding vector size
    // This allows to compute for the output size,  MxW where W is the number of weights (features) to use.
    void setData(py::array_t<double> embedding) {

        std::cout << "create Node 1 for node " << this->name << " ...\n";
        std::cout << embedding << "\n";

        // Convert values to C++ array
        py::buffer_info values_info = embedding.request();
        double* data = static_cast<double*>(values_info.ptr);
        int v_rows = values_info.shape[0]; // N
        int v_cols = values_info.shape[1]; // M
        // Convert a py::array_t row-major order to an Eigen::MatrixXd column-major order.
        this->input_data = Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>(data, v_rows, v_cols);

        std::cout << "create Node 2 for node " << this->name << " ...\n";
        std::cout << input_data << "\n";
    }

    Eigen::MatrixXd getInput() {
        return input_data;
    }

    Eigen::MatrixXd getOutput() {
        return output_data;
    }

    void addInput(Node* input) {
        inputs.insert(input);
        input->outputs.insert(this);
    }

    void addOutput(Node* output) {
        outputs.insert(output);
        output->inputs.insert(this);
    }

    std::unordered_set<Node*> getOutputs() {
        return outputs;
    }

    std::unordered_set<Node*> getInputs() {
        return inputs;
    }

    Node& setOperations(std::vector<std::shared_ptr<BaseOperator>>& operations) {
        this->operations = operations;
        return *this;
    }

    void setReduction(std::string& reducttype) {
        this->reduce = reducttype;
    }

    void sequential(ssize_t repeat) {
        if (repeat > 1) {
            this->repeat = repeat;
        }
    }

    void parallel(ssize_t repeat, std::string& reduce) {
        if (repeat > 1) {
            this->repeat = repeat;
        }
        this->reduce = reduce;
    }

    /*
    // cache Output Data (aggregate if multiple inputs)
    void cacheOutput(Eigen::MatrixXd& output) {

        Eigen::MatrixXd result = output;

        // Note: if Reduction is not specified, it will use average operation as default.
        if (input_data.rows() != 0 && input_data.cols() != 0) {
            if (!(operations).empty()) {
                if (auto reduct = std::dynamic_pointer_cast<Reduction>((this->operations).front())) {
                    input_data = reduct->forward(input_data, output);
                    return;
                } 
            }
            auto reduct = new Reduction("avg");
            input_data = reduct->forward(input_data, output);
        }  else {
            input_data  = output;
        }
    }

    // cache Output Data (aggregate if multiple inputs)
    Eigen::MatrixXd cacheGradient(Eigen::MatrixXd& gradients) {

        Eigen::MatrixXd dInput = gradients;

        // Note: if Reduction is not specified, it will use average operation as default.
        if (!(operations).empty()) {
            if (auto reduct = std::dynamic_pointer_cast<Reduction>((this->operations).front())) {
                dInput = reduct->backward(gradients);
            } 
        }
        return dInput;
    }
    */

    Eigen::MatrixXd aggregateData(Eigen::MatrixXd& input_data) {
        // start with any input_data we keep.
        Eigen::MatrixXd output = input_data;
        // Now add any output data from any source Node that connects to this node.
        std::cout << "Aggregating Data ...\n";

        if (inputs.size() == 0) {
            return input_data;
        }

        std::cout << "Getting size of node input: " << output.size() << " \n";

        for (Node* node : inputs) {
            Eigen::MatrixXd outputx = node->getOutput();
            std::cout << "Getting size of node [" << node->getName() << "] output: " << outputx.size() << " \n";
            std::cout << outputx << "\n";
            if (output.size() == 0) {
                output = outputx;
                continue;
            }
            if (reduce == "add" || reduce == "avg") {
                std::cout << "Aggregating Data by add or average ...\n";
                // change to use more efficient element-wise function which uses GPU/TPU.
                output = output.array() + outputx.array();
            } else
            if (reduce == "mul") {
                std::cout << "Aggregating Data by mul ...\n";
                // change to use more efficient element-wise function which uses GPU/TPU.
                output = output.array() * outputx.array();
            } else
            if (reduce == "matmul") {
                std::cout << "Aggregating Data by matmul ...\n";
                // uses cblas_dgemm
                output = BaseOperator::matmul(output, outputx);
            }

        }
        if (reduce == "avg") {
            // change to use more efficient element-wise function which uses GPU/TPU.
            std::cout << "Aggregating Data by average ...\n";
            output = output.array() / inputs.size();
        }
        std::cout << "Aggregated ...\n";
        std::cout << output << "\n";
        return output;
    }

    void setGradients(Eigen::MatrixXd gradients) {
        dInput = gradients;
    }

    void propagateGradients(Eigen::MatrixXd& gradients) {
        // Now handle all other gradients for other inputs.
        if (inputs.size() != 0)
        for (Node* node : inputs) {
            if (reduce == "add") {
                node->setGradients(gradients);
            } else
            if (reduce == "avg") {
                node->setGradients(gradients.array() / outputs.size());
            } else
            if (reduce == "mul") {
                // change to use more efficient element-wise function which uses GPU/TPU.
                Eigen::MatrixXd dInput = gradients;
                for (Node* nodex : outputs) {
                    if (nodex->getName() != node->getName()) {
                        dInput = dInput.array() * nodex->getOutput().array();
                    }
                }
                node->setGradients(dInput);
            } else
            if (reduce == "matmul") {
                // change to use more efficient element-wise function which uses GPU/TPU.
                Eigen::MatrixXd dInput = gradients;
                for (Node* nodex : outputs) {
                    if (nodex->getName() != node->getName()) {
                        dInput = BaseOperator::matmul(dInput, nodex->getOutput());
                    }
                    dInput = dInput.transpose();
                }
                node->setGradients(dInput);
            }
        }
    }

    // Because of Kahn Algorithm done (see Graph), this function runs forward pass only to 
    // nodes whose source nodes are already processed.
    Eigen::MatrixXd forwardPass() {
        // Propagate forward data to connected nodes
        int size = operations.size();
        print_string(name + " forward pass ...", true);
        std::cout << " operation size: " << size << "\n";

        // See if we can perform reduction.
        Eigen::MatrixXd output = aggregateData(input_data);

        for (const auto& op : operations ) {
                // Check the dynamic type of the object using dynamic_cast
            if (auto linear = std::dynamic_pointer_cast<Linear>(op)) {
                print_string("Linear object", true);
                output = linear->forward(output);
                std::cout << output << std::endl;
            } else
            if (auto batchnorm = std::dynamic_pointer_cast<BatchNorm>(op)) {
                print_string("Batchnorm object", true);
                output = batchnorm->forward(output);
                std::cout << output << std::endl;
            } else            
            if (auto layernorm = std::dynamic_pointer_cast<LayerNorm>(op)) {
                print_string("Layernorm object", true);
                output = layernorm->forward(output);
                std::cout << output << std::endl;
            } else           
            if (auto activate = std::dynamic_pointer_cast<Activation>(op)) {
                print_string("Activate object", true);
                output = activate->forward(output);
                std::cout << output << std::endl;
            } else           
            if (auto attention = std::dynamic_pointer_cast<Attention>(op)) {
                print_string("Attention object", true);
                output = attention->forward(output);
                std::cout << output << std::endl;
            } else           
            if (auto feedforward = std::dynamic_pointer_cast<FeedForward>(op)) {
                print_string("FeedForward object", true);
                output = feedforward->forward(output);
                std::cout << output << std::endl;
            } else           
            if (auto encoder = std::dynamic_pointer_cast<Encoder>(op)) {
                print_string("Encoder object", true);
                output = encoder->forward(output);
                std::cout << output << std::endl;
            }
        }
        this->output_data = output;
        return this->output_data;
    }

    void backwardPass() {
        // Propagate backward gradients to connected nodes
        int size = operations.size();
        print_string(name + " backward pass ...", true);
        std::cout << " operation size: " << size << "\n";

        // Create a copy of the original vector
        std::vector<std::shared_ptr<BaseOperator>> reversedOperations = operations;

        // Reverse the elements in the copied vector
        std::reverse(reversedOperations.begin(), reversedOperations.end());

        // Here, dInput is assumed to have already been propagated
        // through setGradients or propagatGradients.
        for (const auto& op : reversedOperations ) {
            if (auto linear = std::dynamic_pointer_cast<Linear>(op)) {
                print_string("Linear object", true);
                dInput = linear->backward(dInput);
                std::cout << dInput << std::endl;
            } else
            if (auto batchnorm = std::dynamic_pointer_cast<BatchNorm>(op)) {
                print_string("Batchnorm object", true);
                dInput = batchnorm->backward(dInput);
                std::cout << dInput << std::endl;
            } else            
            if (auto layernorm = std::dynamic_pointer_cast<LayerNorm>(op)) {
                print_string("Layernorm object", true);
                dInput = layernorm->backward(dInput);
                std::cout << dInput << std::endl;
            } else           
            if (auto activate = std::dynamic_pointer_cast<Activation>(op)) {
                print_string("Activate object", true);
                dInput = activate->backward(dInput, this->output_data);
                std::cout << dInput << std::endl;
            } else           
            if (auto attention = std::dynamic_pointer_cast<Attention>(op)) {
                print_string("Attention object", true);
                dInput = attention->backward(dInput);
                std::cout << dInput << std::endl;
            } else           
            if (auto feedforward = std::dynamic_pointer_cast<FeedForward>(op)) {
                print_string("FeedForward object", true);
                dInput = feedforward->backward(dInput);
                std::cout << dInput << std::endl;
            } else           
            if (auto encoder = std::dynamic_pointer_cast<Encoder>(op)) {
                print_string("Encoder object", true);
                dInput = encoder->backward(dInput);
                std::cout << dInput << std::endl;
            } 
        }

        // Propagate gradients to next nodes.
        std::cout << "This node: " << this->getName() << " propagating ...\n";
        std::cout << dInput << "\n";
        propagateGradients(dInput);

        // Reinitialize dInput for next EPOCH, as long as parameter gradients have been preserved.
        dInput.setZero();

    }


    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        std::cout << "************************************ Node: " << this->getName() << " ...\n";
        for (const auto& op : operations ) {
                // Check the dynamic type of the object using dynamic_cast
            if (auto linear = std::dynamic_pointer_cast<Linear>(op)) {
                print_string("Linear object", true);
                linear->updateParameters(optimizertype, learningRate, iter);
            } else
            if (auto batchnorm = std::dynamic_pointer_cast<BatchNorm>(op)) {
                print_string("Batchnorm object", true);
                batchnorm->updateParameters(optimizertype, learningRate, iter);
            } else            
            if (auto layernorm = std::dynamic_pointer_cast<LayerNorm>(op)) {
                print_string("Layernorm object", true);
                layernorm->updateParameters(optimizertype, learningRate, iter);
            } else            
            if (auto attention = std::dynamic_pointer_cast<Attention>(op)) {
                print_string("Attention object", true);
                attention->updateParameters(optimizertype, learningRate, iter);
            } else            
            if (auto feedforward = std::dynamic_pointer_cast<FeedForward>(op)) {
                print_string("FeedForward object", true);
                feedforward->updateParameters(optimizertype, learningRate, iter);
            } else            
            if (auto encoder = std::dynamic_pointer_cast<Encoder>(op)) {
                print_string("Encoder object", true);
                encoder->updateParameters(optimizertype, learningRate, iter);
            }
        }
    }

    Eigen::MatrixXd accumulateGradients(Eigen::MatrixXd& gradients) {
        return gradients;
    }

/*
    py::array_t<double> getGradientData() {
        py::array_t<double> result({10, 20});
        return result;
    }
*/
    double* getGradientData() {
        double* matrix = allocate_matrix(20, 30);
        return matrix;
    }

/*
    Eigen::MatrixXd softmax(const Eigen::MatrixXd& input) {
        Eigen::MatrixXd expValues = input.array().exp();
        Eigen::VectorXd sumExp = expValues.rowwise().sum();
        return expValues.array().rowwise() / sumExp.array();
    }
*/

    double computeLoss(const std::vector<double>& target) {
/*
        // Check if the node's output and the target have compatible sizes
        if (output.size() != target.size()) {
            throw std::runtime_error("Output and target sizes mismatch.");
        }

        // Compute the loss based on the node's output and the target values
        double sumSquaredDiff = 0.0;
        for (std::size_t i = 0; i < output.size(); ++i) {
            double diff = output[i] - target[i];
            sumSquaredDiff += diff * diff;
        }
        double loss = sumSquaredDiff / output.size();

        // Set the computed loss as the new value
        setLoss(loss);
*/
        return (double) 0;
    }

    double getLoss() const {
       // return loss;
       return 0;
    }

    void setLoss(double newLoss) {
       // loss = newLoss;
    }

/*
    std::vector<OperationParams> getParameters() const {
        // Retrieve the updated parameters from the node and return them
        return parameters;
    }
*/



    void updateParameters1() {

        // Multiply learning rate and gradients and record the result back to gradients
        // then perform subtraction
        // cblas_dscal(g_rows * g_cols, learningRate, (double*) gradients.data(), 1);
        // parameters.array() -=  gradients.array();

        // Perform direct multiply and subtract operation
    /*
        for (ssize_t i = 0; i < (ssize_t) operations.size(); ++i) {
            Eigen::MatrixXd& weights = parameters[i].weights;
            Eigen::VectorXd& biases = parameters[i].biases;
            Eigen::MatrixXd& gradient = gradients[i];

            // Update the parameters using the gradients
            weights -= learningRate * gradient;
            biases -= learningRate * gradient.rowwise().sum();

            // Reset the gradients to zero for the next iteration
            gradient.setZero();
        }
     */
    }

    ssize_t getParameterSize() const {
        // Compute the total size of parameters for the node
        ssize_t totalSize = 0;
    /*
        for (const auto& parameter : parameters) {
            totalSize += parameter.weights.size();
            totalSize += parameter.biases.size();
        }
    */
        return totalSize;
    } 

    ssize_t getGradientSize() {
       // Compute the total size of parameters for the node
        std::size_t totalSize = 0;
    /*
        for (const auto& gradient : gradients) {
            totalSize += gradient.size();
        }
    */
        return totalSize;
    }
 

    int getId() const {
       return id;
    }


    void printValues() {
/*
        print_string(name + " values: ", false);
        double* mat = NULL;
        for (const auto& values : cache) {
            print_string("operation:", true);
            mat = (double*) values.data();
            for (ssize_t i = 0; i < (ssize_t) values.rows(); i++) {
                for (ssize_t j = 0; j < (ssize_t) values.cols(); j++) {
                    print_double((double) mat[i * values.cols() + j], false);
                }
                print_string("", true);
            }
        }
*/
    }

    void printGradients() {
/*
        print_string(name + " gradients: ", false);
        double* mat = NULL;
        for (const auto& gradient : gradients) {
            print_string("operation:", true);
            mat = (double*) gradient.data();
            for (ssize_t i = 0; i < (ssize_t) gradient.rows(); i++) {
                for (ssize_t j = 0; j < (ssize_t) gradient.cols(); j++) {
                    print_double((double) mat[i * gradient.cols() + j], false);
                }
                print_string("", true);
            }
        }
*/
    }


};

class Connection {
private:
    Node* source;
    Node* destination;

public:
    Connection(Node* sourceNode, Node* destinationNode) : source(sourceNode), destination(destinationNode) {}

    Node* getSource() {
        return source;
    }

    Node* getDestination() {
        return destination;
    }

    void forwardPass() {
        // Eigen::MatrixXd output = source->getOutput();
        //destination->cacheOutput(output);

        // std::cout << "Passing forward connection (reduction) data ...\n";
        // std::cout << output << "\n";

    }

    Eigen::MatrixXd backwardPass(Eigen::MatrixXd& gradients) {

        // Eigen::MatrixXd dInput = destination->cacheGradient(gradients);
        //std::cout << "Passing backward connection (reduction) gradient ...\n";

        return gradients;
    }

};


class Graph {
private:
    std::vector<Node*> nodes;
    std::vector<Connection*> connections;
    std::unordered_map<Node*, int> indegree;
    std::unordered_map<Node*, int> outdegree;
    Eigen::MatrixXd input_data;
    Eigen::MatrixXd predicted;
    Eigen::MatrixXd target;
    Eigen::MatrixXd loss;

public:

    // Create a node with three arguments: name, type, and initial values
    Node* createNode(const std::string& name, NodeType type, const py::array_t<double>& embedding) {
        Node* node = new Node(name, type, embedding);
        nodes.push_back(node);
        return node;
    }

    // Create a node with two arguments: name and type (no initial values)
    Node* createNode(const std::string& name, NodeType type) {
        Node* node = new Node(name, type);
        nodes.push_back(node);
        return node;
    }

    void connect(Node* from, Node* to) {
        addConnection(new Connection(from, to));
    }

    void connect(Node* from, Node* to, std::vector<std::shared_ptr<BaseOperator>>& operations) {
        to->setOperations(operations);
        addConnection(new Connection(from, to));
    }

    void connect(std::vector<Node*> from_nodes, Node* to) {
        for (Node* from : from_nodes) {
            addConnection(new Connection(from, to));
        }
    }

    void connect(std::vector<Node*> from_nodes, Node* to, std::vector<std::shared_ptr<BaseOperator>>& operations) {
        to->setOperations(operations);
        for (Node* from : from_nodes) {
            addConnection(new Connection(from, to));
        }
    }

    void addConnection(Connection* connection) {
        connections.push_back(connection);
        Node* from = connection->getSource();
        Node* to = connection->getDestination();
        outdegree[from]++;
        indegree[to]++;

        to->addInput(from);
        from->addOutput(to);

        std::cout << "Adding Connection\n";
        std::cout << "Nodes size:" << nodes.size() << "\n";
    }

    std::vector<Node*> getNodes() {
        return nodes;
    }

/*
    Eigen::MatrixXd forwardPropagationX() {
        std::unordered_set<Node*> visited;

        Eigen::MatrixXd output = input_data;

        for (Node* node : nodes) {
            if (visited.find(node) == visited.end()) {
                output = forwardPropagationDFS(node, visited, output);
            }
        }
        std::cout << "Final Outcome" << std::endl;
        std::cout << output << std::endl;
        return output;
    }

    Eigen::MatrixXd forwardPropagationDFS(Node* node, std::unordered_set<Node*>& visited, Eigen::MatrixXd& input_data) {
        visited.insert(node);
        Eigen::MatrixXd output = node->forwardPass(input_data);
        for (Node* output_node : adjacencyList[node]) {
            if (visited.find(output_node) == visited.end()) {
                output = forwardPropagationDFS(output_node, visited, output);
            }
        }
        return output;
    }

    void backwardPropagationX() {
        std::unordered_set<Node*> visited;
        for (Node* node : nodes) {
            if (visited.find(node) == visited.end()) {
                backwardPropagationDFS(node, visited);
            }
        }
    }

    void backwardPropagationDFS(Node* node, std::unordered_set<Node*>& visited) {
        visited.insert(node);
        node->backwardPass();
        for (Node* input : reverseAdjacencyList[node]) {
            if (visited.find(input) == visited.end()) {
                backwardPropagationDFS(input, visited);
            }
        }
    }


    std::vector<std::vector<Node*>> splitStageNodes(const std::vector<Node*>& stageNodes, int numModelParallel) {
        int numNodes = stageNodes.size();
        int nodesPerSubset = numNodes / numModelParallel;
        int remainingNodes = numNodes % numModelParallel;

        std::vector<std::vector<Node*>> subsets;
        subsets.reserve(numModelParallel);

        int startIndex = 0;
        for (int i = 0; i < numModelParallel; ++i) {
             int subsetSize = nodesPerSubset + (i < remainingNodes ? 1 : 0);
             subsets.push_back(std::vector<Node*>(stageNodes.begin() + startIndex, stageNodes.begin() + startIndex + subsetSize));
             startIndex += subsetSize;
        }

        return subsets;
    }

    void gatherGradients(const std::vector<Node*>& nodes, std::vector<double>& aggregatedGradients) {
        // int numNodes = nodes.size();
        int totalSize = 0;

        // Compute total size of aggregated gradients and gather gradients
       
        for (int i = 0; i < numNodes; ++i) {
            int gradientSize = nodes[i]->getGradientSize();
            std::memcpy(&aggregatedGradients[totalSize], (double*)nodes[i]->getGradientData(), gradientSize * sizeof(double));
            totalSize += gradientSize;
        }

        MPI_Gather(aggregatedGradients.data(), totalSize, MPI_DOUBLE, aggregatedGradients.data(), totalSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    }


    void gatherGradients(const std::vector<Node*>& nodes, std::vector<double>& aggregatedGradients) {
        int numNodes = nodes.size();
        int gradientSize = nodes[0]->getGradientSize();
        std::vector<double> gradients(gradientSize * numNodes);

        for (int i = 0; i < numNodes; ++i) {
             std::memcpy(&gradients[i * gradientSize], (double*) nodes[i]->getGradientData(), gradientSize * sizeof(double));
        }

        MPI_Gather(gradients.data(), gradientSize * numNodes, MPI_DOUBLE, aggregatedGradients.data(), gradientSize * numNodes, MPI_DOUBLE, 0, MPI_COMM_WORLD);
    }
*/

/*
    void aggregateLosses(const std::vector<Node*>& subset) {
        // Perform aggregation of losses within the stage
        // This could involve summing the losses or other appropriate aggregation operations
        // Implementation depends on the specific requirements of your model and pipeline parallelism strategy
    }


    void aggregateLosses(const std::vector<Node*>& subset) {
        for (Node* node : subset) {
            double loss = node->getLoss();
            double aggregatedLoss = 0.0;

            // Use MPI_Reduce to aggregate the losses from all nodes
            MPI_Reduce(&loss, &aggregatedLoss, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

            if (node->getId() == 0) {
                // Set the aggregated loss to the first node in the subset
                node->setLoss(aggregatedLoss);
            }
        }
    }

    void synchronizeStage() {
        // Perform synchronization within the stage (e.g., using OpenMP barrier)
        #pragma omp barrier
    }

    // Perform topological sorting on the DAG
    std::vector<Node*> topologicalSort(std::vector<Node*>& nodes) {
        std::vector<Node*> sortedNodes;
        std::unordered_set<Node*> visited;

        for (Node* node : nodes) {
            if (!visited.count(node)) {
                topologicalSortUtil(node, visited, sortedNodes);
            }
        }

        // std::reverse(sortedNodes.begin(), sortedNodes.end());
        return sortedNodes;
    }

    // Helper function for topological sort
    void topologicalSortUtil(Node* node, std::unordered_set<Node*>& visited, std::vector<Node*>& sortedNodes) {
        visited.insert(node);

        for (Node* output : node->getOutputs()) {
            if (!visited.count(output)) {
                topologicalSortUtil(output, visited, sortedNodes);
            }
        }

        sortedNodes.push_back(node);
    }

    // Group the nodes into regions for parallel and serial execution
    std::vector<std::vector<Node*>> groupNodes(const std::vector<Node*>& sortedNodes) {
        std::vector<std::vector<Node*>> regions;
        std::vector<Node*> currentRegion;
        std::unordered_set<Node*> visited;

        for (Node* node : sortedNodes) {
            if (node->getInputs().size() > 1 || node->getOutputs().size() > 1) {
                // Add the node to the current parallel region
                currentRegion.push_back(node);
            } else {
                // Start a new serial region
                if (!currentRegion.empty()) {
                    regions.push_back(currentRegion);
                    currentRegion.clear();
                }

                // Add the node to the current serial region
                currentRegion.push_back(node);

                // Start a new parallel region
                if (node->getOutputs().size() > 1) {
                    regions.push_back(currentRegion);
                    currentRegion.clear();
                }
            }
        }

        // Add the last region if it is not empty
        if (!currentRegion.empty()) {
            regions.push_back(currentRegion);
        }

        return regions;
    }

  

    // Assuming you have the following vectors representing your regions, repeatSequentially, and repeatCounts
    // Example use:
    //    std::vector<std::vector<Node*>> regions = {region1, region2, region3};
    //    std::vector<bool> repeatSequentially = {true, false, true};
    //    std::vector<int> repeatCounts = {3, 1, 2};
    //
    //    // Call the repeatRegions function
    //    std::vector<std::vector<Node*>> repeatedRegions = repeatRegions(regions, repeatSequentially, repeatCounts);
    std::vector<std::vector<Node*>> repeatRegions(const std::vector<std::vector<Node*>>& regions, const std::vector<bool>& repeatSequentially, const std::vector<int>& repeatCounts) {
        std::vector<std::vector<Node*>> repeatedRegions;

        print_string("Repeat Regions:", false);
        print_double((double) regions.size(), true);
        
        for (size_t i = 0; i < regions.size(); i++) {
            if (repeatSequentially[i]) {
                int repeatCount = repeatCounts[i];
                while (repeatCount > 0) {
                    repeatedRegions.push_back(regions[i]);
                    repeatCount--;
                }
            } else {
                repeatedRegions.push_back(regions[i]);
            }
        }
        
        return repeatedRegions;
    }

      */

/*
    void executeRegionForward(const std::vector<Node*>& region, int numProcesses, int rank) {
        int numNodes = region.size();
        // Split the nodes evenly among processes
        int nodesPerProcess = numNodes / numProcesses;
        int startNodeIndex = rank * nodesPerProcess;
        int endNodeIndex = (rank == numProcesses - 1) ? numNodes : startNodeIndex + nodesPerProcess;

        // Execute nodes assigned to the current process
        for (int i = startNodeIndex; i < endNodeIndex; ++i) {
            // Execute the forward pass of the node
            region[i]->forwardPass();
        }
        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);
    }

    void executeRegionComputeLoss(const std::vector<Node*>& region, std::vector<double>& target, int numProcesses, int rank) {
        int numNodes = region.size();
        // Split the nodes evenly among processes
        int nodesPerProcess = numNodes / numProcesses;
        int startNodeIndex = rank * nodesPerProcess;
        int endNodeIndex = (rank == numProcesses - 1) ? numNodes : startNodeIndex + nodesPerProcess;

        // Execute nodes assigned to the current process
        // Compute loss and aggregate across processes
        double localLoss = 0.0;
        for (int i = startNodeIndex; i < endNodeIndex; ++i) {
            // Execute the forward pass of the node
            localLoss += region[i]->computeLoss(target);
        }

        double globalLoss = 0.0;
        MPI_Allreduce(&localLoss, &globalLoss, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

        if (rank == 0) {
            std::cout << "Global Loss for Stage " << globalLoss << std::endl;
        }

        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);
    }

    void executeRegionBackward(const std::vector<Node*>& region, int numProcesses, int rank) {
        int numNodes = region.size();
        // Split the nodes evenly among processes
        int nodesPerProcess = numNodes / numProcesses;
        int startNodeIndex = rank * nodesPerProcess;
        int endNodeIndex = (rank == numProcesses - 1) ? numNodes : startNodeIndex + nodesPerProcess;

        // Execute nodes assigned to the current process
        for (int i = startNodeIndex; i < endNodeIndex; ++i) {
            // Execute the forward pass of the node
            region[i]->backwardPass();
        }
        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);
    }

    void executeRegionUpdateParameters(const std::vector<Node*>& region, int numProcesses, int rank) {
        int numNodes = region.size();
        // Split the nodes evenly among processes
        int nodesPerProcess = numNodes / numProcesses;
        int startNodeIndex = rank * nodesPerProcess;
        int endNodeIndex = (rank == numProcesses - 1) ? numNodes : startNodeIndex + nodesPerProcess;

        // Execute nodes assigned to the current process
        for (int i = startNodeIndex; i < endNodeIndex; ++i) {
            // Execute the forward pass of the node
            region[i]->updateParameters();
        }
        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);
    }

    void executePipelineParallel1(const  std::vector<std::vector<Node*>>& repeatedRegions) {
        // Get the number of MPI processes and rank
        int numProcesses, rank;
        MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);

        // Execute regions in sequence.
        for (const auto& region : repeatedRegions) {
            executeRegionForward(region, numProcesses, rank);
        }

        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);

        // Execute regions in sequence.
        std::vector<double> target;
        for (const auto& region : repeatedRegions) {
            executeRegionComputeLoss(region, target, numProcesses, rank);
        }

        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);

        // Execute regions in sequence.
        for (const auto& region : repeatedRegions) {
            executeRegionBackward(region, numProcesses, rank);
        }

        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);

        // Execute regions in sequence.
        for (const auto& region : repeatedRegions) {
            executeRegionUpdateParameters(region, numProcesses, rank);
        }

        // Synchronize all processes to ensure all nodes in the region are executed
        MPI_Barrier(MPI_COMM_WORLD);
    }
*/


    // Perform the Kahn's Algorithm by Arthur B. Khan based on his 1962 paper, "Topological Sorting of Large Networks"
    Eigen::MatrixXd forwardPropagation() {
        std::queue<Node*> q;
        std::unordered_map<Node*, int> indegree_(indegree); 

        std::cout << "Entering Forward Pass.\n";

        std::cout << "Size:" << getNodes().size() << "\n";

        for (Node* node : nodes) {
            if (indegree[node] == 0) {
                q.push(node);
            }
        }

        std::cout << "Collecting nodes for the queue.\n";
        std::cout << "Size of queue: " << q.size() << "\n";

        Eigen::MatrixXd output(0, 0);

        std::cout << "Entering Queue Loop.\n";

        while (!q.empty()) {
            Node* node = q.front();
            q.pop();

            // Eigen::MatrixXd input_data = node->getInput();

            std::cout << "*** Entering forward pass for " << node->name << " **** \n";

             // output = node->forwardPass(input_data);     
            output = node->forwardPass();     

            std::cout << "Processing completed for node [" << node->name << "]\n";
            // std::cout << output << std::endl;

            for (Connection* connection : connections) {
                if (connection->getSource() == node) {

                    //std::cout << "Processing completed for connection.\n";
                    //connection->forwardPass();
            
                    // Propagate gradient to source node
                    Node* dstNode = connection->getDestination();

                    indegree_[dstNode]--;
                    if (indegree_[dstNode] == 0) {
                        q.push(dstNode);
                    }
                }
            }
        }

        std::cout << "Forward Processing completed for all nodes.\n";

        return output;
    }

    Eigen::MatrixXd backwardPropagation(Eigen::MatrixXd& gradients) {
        std::queue<Node*> q;
        std::unordered_map<Node*, int> outdegree_(outdegree); 

        std::cout << "\n\nEntering Backward Pass ***************************************\n";

        std::cout << "Size:" << getNodes().size() << "\n";

        for (Node* node : nodes) {
            if (outdegree[node] == 0) {
                node->setGradients(gradients); // gradients with respect to loss function.
                q.push(node);
            }
        }

        std::cout << "Collecting nodes for the queue.\n";
        std::cout << "Size of queue: " << q.size() << "\n";

        std::cout << "Entering Queue Loop.\n";

        while (!q.empty()) {
            Node* node = q.front();
            q.pop();

            std::cout << "*** Entering backward pass for " << node->name << " **** \n";

            // Compute gradients for the node
            node->backwardPass();

            std::cout << "Processing completed for node [" << node->name << "]\n";

            for (Connection* connection : connections) {
                if (connection->getDestination() == node) {

                    //std::cout << "Processing completed for connection.\n";
                    //Eigen::MatrixXd srcGradients = connection->backwardPass(gradients);

                    // Propagate gradient to source node
                    Node* srcNode = connection->getSource();

                    outdegree_[srcNode]--;
                    if (outdegree_[srcNode] == 0) {
                        q.push(srcNode);
                    }
                }
            }
        }

        std::cout << "Backward Processing completed for all nodes.\n";

        return gradients;
    }

    Eigen::MatrixXd computeLoss(std::string losstype, Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Loss* obj = new Loss(losstype);
        Eigen::MatrixXd loss = obj->computeLoss(predicted, target);
        std::cout << "Loss calculated ... \n";
        std::cout << loss << "\n";
        return loss;
    }

    Eigen::MatrixXd computeGradients(std::string losstype, Eigen::MatrixXd& predicted, const Eigen::MatrixXd& target) {
        Loss* obj = new Loss(losstype);
        Eigen::MatrixXd gradients = obj->computeGradients(predicted, target);
        std::cout << "Loss Gradient calculated ... \n";
        // std::cout << gradients << "\n";
        return gradients;
    }

    void updateParameters(std::string& optimizertype, double& learningRate, int& iter) {
        for (Node* node: nodes) {
            node->updateParameters(optimizertype, learningRate, iter);
        }
    }

    void nextBatch() {
        for (Node* node: nodes) {
          if (node->nodeType() == NodeType::Input)  {

          }
        }
    }

    const std::unordered_map<Node*, int>& getIndegree() const {
        return indegree;
    }

};

class Model {
private:
    Graph* graph;
    Eigen::MatrixXd target;
    Eigen::MatrixXd predicted;
    Eigen::MatrixXd loss;
    Eigen::MatrixXd gradients;
    std::string losstype = "mse";
    std::string optimizertype = "adam";
    double learningRate = 0.01;
    int itermax = 1;
public:

    Model(const std::string& losstype = "mse", const std::string& optimizertype = "adam",
          const double learningRate = 0.01, const int itermax = 1) {
        this->losstype = losstype;
        this->optimizertype = optimizertype;
        this->learningRate = learningRate;
        this->itermax = itermax;
    }

    void setGraph(Graph* graph) {

        auto mygraph = dynamic_cast<Graph*>(graph);
        this->graph = mygraph;
        std::cout << "(setGraph) See the size ...\n";
        std::cout << this->graph->getNodes().size() << " all size account ... \n";
    }

    void setLoss(std::string& losstype) {
        this->losstype = losstype;
    }

    // The input is assumed to have NxM where N=number of samples, M=embedding vector size
    // This allows to compute for the output size,  MxW where W is the number of weights (features) to use.
    void setTarget(py::array_t<double> target) {

        std::cout << "Set Target ...\n";
        std::cout << target << "\n";

        // Convert values to C++ array
        py::buffer_info values_info = target.request();
        double* data = static_cast<double*>(values_info.ptr);
        int v_rows = values_info.shape[0]; // N
        int v_cols = values_info.shape[1]; // M
        // Convert a py::array_t row-major order to an Eigen::MatrixXd column-major order.
        this->target = Eigen::Map<Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>>(data, v_rows, v_cols);

        std::cout << this->target << "\n";
    }

    Eigen::MatrixXd getTarget() {
        return target;
    }

    void useCrossEntropy() {

    }

    void train(std::string& losstype, std::string& optimizertype, double learnrate = 0.01, int itermax = 1) {

            // Initialize MPI
        //MPI_Init(NULL, NULL);

        this->losstype = losstype;
        this->optimizertype = optimizertype;
        this->learningRate = learnrate;

        std::cout << "(train) See the size ...\n";
        std::cout << this->graph->getNodes().size() << " all size account ... \n";

        double epsilon = 1e-3;
        double old_loss = inf();

        int iter = 0;

        auto start_time = std::chrono::system_clock::now();

        do {

            std::cout << "<<<<<<<<<<<<<<<<<<<<<<<<< **************** Process batch (iteration " << (iter+1) << ") ************* >>>>>>>>>>>>>>>>>>>>>>>>>> \n";
            this->graph->nextBatch();

            std::cout << "Entering Forward Propagation ...\n";
            predicted = this->graph->forwardPropagation();

            std::cout << "Predicted Result: \n";
            std::cout << predicted << "\n";

            std::cout << "Compute Loss ...\n";
            loss = this->graph->computeLoss(losstype, predicted, target);

            auto end_time = std::chrono::system_clock::now();
            std::chrono::duration<double> elapsed_seconds = end_time - start_time;
            std::time_t next_time = std::chrono::system_clock::to_time_t(end_time);
            start_time = end_time;
            std::cout << "---------------------------------------------------------------------------------> Loss:" 
                      << loss 
                      << " ... elapsed " <<  elapsed_seconds.count() 
                      << " at " << std::ctime(&next_time) << "\n";

            std::cout << "Compute Gradient ...\n";
            gradients = this->graph->computeGradients(losstype, predicted, target);
            std::cout << gradients << "\n";

            std::cout << "Entering Backward Propagation ...\n";
            gradients = this->graph->backwardPropagation(gradients); 

            std::cout << "\n\n\n";
            std::cout << "Updating Parameters ...\n";
            this->graph->updateParameters(this->optimizertype, this->learningRate, iter);

            iter ++;

            if (iter >= itermax) break; 



        } while (abs(old_loss - loss(0,0)) > epsilon);

        std::cout << "Training done ...\n";

        // Finalize MPI
        //MPI_Finalize();

    }

};



/*
void trainModel(const py::list& repeatSequentially, const py::list& repeatCounts) {

    std::vector<bool> repeatSeq;
    std::vector<int> repeatCnt;

    for (const auto& item : repeatSequentially) {
        repeatSeq.push_back(py::cast<bool>(item));
    }

    for (const auto& item : repeatCounts) {
        repeatCnt.push_back(py::cast<int>(item));
    }

    std::vector<Node*> nodes = this->getNodes();

    // Sort nodes topologically
    std::vector<Node*> sortedNodes = topologicalSort(nodes);

    // Group sorted nodes into sequential regions
    std::vector<std::vector<Node*>> regions = groupNodes(sortedNodes);

    // Repeat regions based on repeatSequentially and repeatCounts
    std::vector<std::vector<Node*>> repeatedRegions = repeatRegions(regions, repeatSeq, repeatCnt);

        // Initialize MPI
    MPI_Init(NULL, NULL);

    // Execute Pipeline
    //executePipelineParallel(repeatedRegions);

    executePipelineParallel();

    // Perform data exchange between regions based on connections
    // performDataExchange(graph, repeatedRegions);

    // Gather gradients from all regions
    // std::vector<double> aggregatedGradients(nodes.size());
    // gatherGradients(repeatedRegions, aggregatedGradients);

    // Aggregate gradients and perform parameter update
    // aggregateGradients(aggregatedGradients);
    // performParameterUpdate(nodes);

    // Finalize MPI
    MPI_Finalize();

}
*/


PYBIND11_MODULE(genai, m) {
    m.doc() = "Example C++ module for Python";

    py::enum_<NodeType>(m, "NodeType")
        .value("Input", NodeType::Input)
        .value("Hidden", NodeType::Hidden)
        .value("Output", NodeType::Output)
        .export_values();

    py::enum_<ReductionType>(m, "ReductionType")
        .value("SUM", ReductionType::SUM)
        .value("AVG", ReductionType::AVG)
        .value("MIN", ReductionType::MIN)
        .value("MAX", ReductionType::MAX)
        .value("ARGMIN", ReductionType::ARGMIN)
        .value("ARGMAX", ReductionType::ARGMAX)
        .value("MATMUL", ReductionType::MATMUL)
        .value("MUL", ReductionType::MUL)
        .export_values();

      py::enum_<ActivationType>(m, "ActivationType")
        .value("SIGMOID", ActivationType::SIGMOID)
        .value("TANH", ActivationType::TANH)
        .value("LEAKYRELU", ActivationType::LEAKYRELU)
        .value("GELU", ActivationType::GELU)
        .value("SOFTMAX", ActivationType::SOFTMAX)
        .export_values();

    py::class_<Node>(m, "Node")
        .def("setData", &Node::setData)
        .def("sequential", &Node::sequential, py::arg("repeat") = 1)
        .def("parallel", &Node::parallel, py::arg("repeat") = 1, py::arg("reduce") = "add" )
        .def("setOperations", (Node& (Node::*)(std::vector<std::shared_ptr<BaseOperator>>&)) &Node::setOperations)
        .def("setReduction", &Node::setReduction, py::arg("type") = "add" )
        .def("forwardPass", &Node::forwardPass)
        .def("backwardPass", &Node::backwardPass)
        .def("printValues", &Node::printValues)
        .def("printGradients", &Node::printGradients);

    py::class_<Graph>(m, "Graph")
        .def(py::init<>())
        .def("addNode", [](Graph& graph, const std::string& name, NodeType type, const py::array_t<double>& embedding) {
            return graph.createNode(name, type, embedding);
        })
        .def("addNode", [](Graph& graph, const std::string& name, NodeType type) {
            return graph.createNode(name, type);
        })
        .def("connect", (void (Graph::*)(Node*,Node*)) &Graph::connect, "Connects this node to another node")
        .def("connect", (void (Graph::*)(Node*,Node*, std::vector<std::shared_ptr<BaseOperator>>&)) &Graph::connect, "Connects this node to another node with functions")
        .def("connect", (void (Graph::*)(std::vector<Node*>, Node*)) &Graph::connect, "Connects this node to multiple nodes")
        .def("connect", (void (Graph::*)(std::vector<Node*>, Node*, std::vector<std::shared_ptr<BaseOperator>>&)) &Graph::connect, "Connects this node to multiple nodes with functions")
        .def("forwardPropagation", &Graph::forwardPropagation)
        .def("backwardPropagation", &Graph::backwardPropagation);


    py::class_<Model>(m, "Model")
        .def(py::init<>())
        .def("setGraph", [](Model& model, Graph* graph)  {
             model.setGraph(graph);
        })
        .def("train", &Model::train, py::arg("loss") = "mse",  
                py::arg("optimizer") = "adam", py::arg("learnrate") = 0.01, 
                py::arg("iter")=1, "Training a model")
        .def("setTarget", &Model::setTarget, "set Target of a model");


    py::class_<BaseOperator, std::shared_ptr<BaseOperator>>(m, "BaseOperator");
    py::class_<Linear, BaseOperator, std::shared_ptr<Linear>>(m, "Linear")
        .def(py::init<int, bool>(), py::arg("size") = 0, py::arg("bias") = true);
    py::class_<BatchNorm, BaseOperator, std::shared_ptr<BatchNorm>>(m, "BatchNorm")
        .def(py::init<int>(), py::arg("size") = 0);
    py::class_<LayerNorm, BaseOperator, std::shared_ptr<LayerNorm>>(m, "LayerNorm")
        .def(py::init<int>(), py::arg("size") = 0); 
    py::class_<Reduction, BaseOperator, std::shared_ptr<Reduction>>(m, "Reduction")
        .def(py::init<const std::string&>(), py::arg("type") = "sum");  
    py::class_<Activation, BaseOperator, std::shared_ptr<Activation>>(m, "Activation")
        .def(py::init<const std::string&, const float>(), py::arg("type") = "relu", py::arg("alpha") = 0.01);
    py::class_<Loss, BaseOperator, std::shared_ptr<Loss>>(m, "Loss")
        .def(py::init<const std::string&>(), py::arg("type") = "mse");
    py::class_<Dropout, BaseOperator, std::shared_ptr<Dropout>>(m, "Dropout")
        .def(py::init<int>(), py::arg("size") = 0); 
    py::class_<Attention, BaseOperator, std::shared_ptr<Attention>>(m, "Attention")
        .def(py::init<int,int, bool>(), py::arg("heads") = 1, py::arg("size") = 3, py::arg("bias") = false); 
    py::class_<FeedForward, BaseOperator, std::shared_ptr<FeedForward>>(m, "FeedForward")
        .def(py::init<int, bool, const std::string&, const float>(), 
                py::arg("size") = 3, py::arg("bias") = true,
                py::arg("type") = "relu", py::arg("alpha") = 0.01);
    py::class_<Encoder, BaseOperator, std::shared_ptr<Encoder>>(m, "Encoder")
        .def(py::init<int, int, bool, const std::string&, const float>(), 
                py::arg("heads") = 1,
                py::arg("size") = 3, py::arg("bias") = true,
                py::arg("type") = "relu", py::arg("alpha") = 0.01);


    // Define function to print hello
    m.def("print_string", &print_string, "Print 'string'");
    m.def("print_double", &print_double, "Print 'double'");
    m.def("process_array", &process_array, "Process a NumPy array");
    m.def("process_matrix", &process_matrix, "Process a NumPy array");
    m.def("matmul", &matmul, "Matrix Multiplication a NumPy array");

    // Set std::cout precision display
    std::cout.precision(12);

 
}
